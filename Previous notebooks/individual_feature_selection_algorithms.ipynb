{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyPM9J21Q4mXJzk9t5V/hJrX"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Askn1naxHZmU",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1737710235305,
     "user_tz": -120,
     "elapsed": 3673,
     "user": {
      "displayName": "תומר וסרמן",
      "userId": "13570990899464465396"
     }
    },
    "outputId": "3919f3e4-2ad3-4959-bbe6-1bca9704beeb",
    "ExecuteTime": {
     "end_time": "2025-01-24T20:28:14.771597Z",
     "start_time": "2025-01-24T20:28:13.188313Z"
    }
   },
   "source": "!pip install ucimlrepo",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: ucimlrepo in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (0.0.7)\n",
      "Requirement already satisfied: pandas>=1.0.0 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from ucimlrepo) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2020.12.5 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from ucimlrepo) (2024.12.14)\n",
      "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from pandas>=1.0.0->ucimlrepo) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\aviv metz\\miniconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas>=1.0.0->ucimlrepo) (1.16.0)\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectKBest"
   ],
   "metadata": {
    "id": "CeNLwRfGHPGj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1737710235305,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "תומר וסרמן",
      "userId": "13570990899464465396"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-01-24T20:28:16.646761Z",
     "start_time": "2025-01-24T20:28:15.929943Z"
    }
   },
   "outputs": [],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:28:17.842421Z",
     "start_time": "2025-01-24T20:28:17.839568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ISOLET_DB_INDEX: int = 54\n",
    "SPAMBASE_DB_INDEX: int = 94"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:28:21.809807Z",
     "start_time": "2025-01-24T20:28:19.091724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Read first dataset\n",
    "isolet = fetch_ucirepo(id=52)\n",
    "\n",
    "features = isolet.data.features\n",
    "target_variables = isolet.data.targets\n"
   ],
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:28:08.881767Z",
     "start_time": "2025-01-24T20:28:08.153674Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from ucimlrepo import dotdict\n",
    "\n",
    "isolet: dotdict\n",
    "isolet"
   ],
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'isolet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[1], line 4\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mfrom\u001B[39;00m \u001B[38;5;21;01mucimlrepo\u001B[39;00m \u001B[38;5;28;01mimport\u001B[39;00m dotdict\n\u001B[0;32m      3\u001B[0m isolet: dotdict\n\u001B[1;32m----> 4\u001B[0m isolet\n",
      "\u001B[1;31mNameError\u001B[0m: name 'isolet' is not defined"
     ]
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:06.341869Z",
     "start_time": "2025-01-24T20:17:06.335851Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "# Print missing values and maximum and minimum values in the features of the first dataset\n",
    "X_df = pd.DataFrame(features)\n",
    "y_df = pd.DataFrame(target_variables)\n",
    "\n",
    "print(\"Missing values in X:\", X_df.isnull().sum().sum())\n",
    "print(\"Missing values in y:\", y_df.isnull().sum().sum())\n",
    "\n",
    "print(\"Minimum value across all features:\", X_df.min().min())\n",
    "print(\"Maximum value across all features:\", X_df.max().max())"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing values in X: 0\n",
      "Missing values in y: 0\n",
      "Minimum value across all features: -1.0\n",
      "Maximum value across all features: 1.0\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "code",
   "source": [
    "# Normalize the first dataset\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_df)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_df.columns)\n",
    "\n",
    "print(X_scaled_df.min().min())\n",
    "print(X_scaled_df.max().max())"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pZ5rMZL_e4Pa",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1737710237868,
     "user_tz": -120,
     "elapsed": 6,
     "user": {
      "displayName": "תומר וסרמן",
      "userId": "13570990899464465396"
     }
    },
    "outputId": "3aba8309-e5c9-4a8c-efed-8c9a223258ae",
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:12.700027Z",
     "start_time": "2025-01-24T20:17:12.694051Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "1.0\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:12.722129Z",
     "start_time": "2025-01-24T20:17:12.719204Z"
    }
   },
   "cell_type": "code",
   "source": "RANDOM_FOREST_SEED: int = 42",
   "outputs": [],
   "execution_count": 7
  },
  {
   "cell_type": "code",
   "source": [
    "# # Read second dataset\n",
    "# spambase = fetch_ucirepo(id=94)\n",
    "\n",
    "# X = spambase.data.features\n",
    "# y = spambase.data.targets"
   ],
   "metadata": {
    "id": "Y3632T4akePZ",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1737710237868,
     "user_tz": -120,
     "elapsed": 5,
     "user": {
      "displayName": "תומר וסרמן",
      "userId": "13570990899464465396"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:12.794331Z",
     "start_time": "2025-01-24T20:17:12.791447Z"
    }
   },
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "code",
   "source": [
    "# # Print missing values and maximum and minimum values in the features of the second dataset\n",
    "\n",
    "# X_df = pd.DataFrame(X)\n",
    "# y_df = pd.DataFrame(y)\n",
    "\n",
    "# print(\"Missing values in X:\", X_df.isnull().sum().sum())\n",
    "# print(\"Missing values in y:\", y_df.isnull().sum().sum())\n",
    "\n",
    "\n",
    "# print(\"Minimum value across all features:\", X_df.min().min())\n",
    "# print(\"Maximum value across all features:\", X_df.max().max())"
   ],
   "metadata": {
    "id": "rEtLDi40khHC",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1737710237868,
     "user_tz": -120,
     "elapsed": 5,
     "user": {
      "displayName": "תומר וסרמן",
      "userId": "13570990899464465396"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:12.801205Z",
     "start_time": "2025-01-24T20:17:12.798318Z"
    }
   },
   "outputs": [],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:12.868478Z",
     "start_time": "2025-01-24T20:17:12.865593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# # Normalize the second dataset\n",
    "# scaler = MinMaxScaler()\n",
    "# X_scaled = scaler.fit_transform(X_df)\n",
    "\n",
    "# X_scaled_df = pd.DataFrame(X_scaled, columns=X_df.columns)\n",
    "\n",
    "# print(X_scaled_df.min().min())\n",
    "# print(X_scaled_df.max().max())\n"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:12.877908Z",
     "start_time": "2025-01-24T20:17:12.875457Z"
    }
   },
   "cell_type": "code",
   "source": [
    "data_after_scaling = X_scaled_df\n",
    "target_variables = y_df.values.ravel()\n"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "cell_type": "code",
   "source": [
    "def get_top_k_features(selector, feature_names: list, top_features_to_select: int, algorithm: str, verbose: bool = False):\n",
    "  \"\"\"\n",
    "  Get the top k features based on their scores from a SelectKBest selector.\n",
    "\n",
    "  Parameters:\n",
    "  selector (SelectKBest): Fitted SelectKBest object.\n",
    "  feature_names (list): List of feature names (columns of X).\n",
    "  k (int): Number of top features to select.\n",
    "  algorithm (str): The name of the feature selection algorithm.\n",
    "\n",
    "  Returns:\n",
    "  list: Names of the top k features.\n",
    "  \"\"\"\n",
    "  # Retrieve feature scores\n",
    "  scores = selector.scores_\n",
    "\n",
    "  # Create a DataFrame for ranked features\n",
    "  feature_ranking = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Score': scores\n",
    "  }).sort_values(by='Score', ascending=False)\n",
    "  if verbose:\n",
    "    # Display top-ranked features\n",
    "    print(f\"Feature Rankings using {algorithm}:\")\n",
    "    print(feature_ranking)\n",
    "\n",
    "\n",
    "  # Return selected top k features\n",
    "  return feature_ranking.head(top_features_to_select)['Feature'].tolist()"
   ],
   "metadata": {
    "id": "qIJoIpIA_uOj",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1737710238141,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "תומר וסרמן",
      "userId": "13570990899464465396"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:12.918182Z",
     "start_time": "2025-01-24T20:17:12.915041Z"
    }
   },
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:12.958831Z",
     "start_time": "2025-01-24T20:17:12.955711Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_fit_random_forest(X_train, X_test, y_train, y_test):\n",
    "  \"\"\"\n",
    "  Builds, trains, and evaluates a Random Forest classification model.\n",
    "\n",
    "  Parameters:\n",
    "  ----------\n",
    "  X_train : pd.DataFrame or np.ndarray\n",
    "      Feature matrix for training the model.\n",
    "  X_test : pd.DataFrame or np.ndarray\n",
    "      Feature matrix for testing the model.\n",
    "  y_train : pd.Series or np.ndarray\n",
    "      Target labels for training the model.\n",
    "  y_test : pd.Series or np.ndarray\n",
    "      True target labels for testing the model.\n",
    "\n",
    "  Returns:\n",
    "  float: The accuracy of the model on the selected features\n",
    "  \"\"\"\n",
    "  # Build a simple classification model\n",
    "  model = RandomForestClassifier(random_state=RANDOM_FOREST_SEED)\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Evaluate the model\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "  # Detailed performance metrics\n",
    "  # print(\"\\nClassification Report:\")\n",
    "  # print(classification_report(y_test, y_pred))\n",
    "\n",
    "  # Return the accuracy of the model\n",
    "  return accuracy\n",
    "\n",
    "\n"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:13.000041Z",
     "start_time": "2025-01-24T20:17:12.996537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_top_features_with_selector(selector: SelectKBest,\n",
    "                                    num_of_features_to_select: int,\n",
    "                                    data_with_features,\n",
    "                                    target_variables,\n",
    "                                    algorithm: str = \"\",\n",
    "                                    verbose: bool = False\n",
    "                                    ):\n",
    "      \"\"\"\n",
    "      :param selector: SelectKBest object.\n",
    "      :param num_of_features_to_select:\n",
    "      :param data_with_features: The features are selected from this data.\n",
    "      :param target_variables: The variable the feature selection is used on.\n",
    "      :param algorithm: The algorithm used, as a str. Used for debug printouts.\n",
    "      :param verbose: Enable debug printouts.\n",
    "      :return:\n",
    "      \"\"\"\n",
    "      selector.fit(data_with_features, target_variables)\n",
    "\n",
    "      # Rank the features using Chi-Square algorithm\n",
    "      top_features = get_top_k_features(selector=selector, feature_names=features.columns,\n",
    "                                        top_features_to_select=num_of_features_to_select, algorithm=algorithm,\n",
    "                                        verbose=verbose)\n",
    "\n",
    "      return data_with_features[top_features]"
   ],
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:13.082289Z",
     "start_time": "2025-01-24T20:17:13.079785Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MIN_FEATURES: int = 5\n",
    "MAX_FEATURES: int = 10\n",
    "TRAIN_TEST_SPLIT_RATIO: float = 0.2"
   ],
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T20:20:27.641221Z",
     "start_time": "2025-01-24T20:20:27.092945Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Apply Chi-Square\n",
    "from sklearn.feature_selection import chi2\n",
    "\n",
    "results = []\n",
    "for k in range(MIN_FEATURES, MAX_FEATURES):\n",
    "  # SelectKBest with chi2 evaluates all features\n",
    "  X_top = get_top_features_with_selector(\n",
    "      selector=SelectKBest(score_func=chi2, k='all'),\n",
    "      num_of_features_to_select=k,\n",
    "      data_with_features=data_after_scaling,\n",
    "      target_variables=target_variables,\n",
    "      algorithm=\"Chi-Square\",\n",
    "      verbose=False\n",
    "  )\n",
    "\n",
    "  # Split the data into train and test sets\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_top, target_variables, test_size=TRAIN_TEST_SPLIT_RATIO, random_state=42)\n",
    "\n",
    "  # Train and fit random forest classification model based on feature selected\n",
    "  print(\"---------------------------------\")\n",
    "  print(f'Amount of features selected: {k}')\n",
    "  accuracy = train_and_fit_random_forest(X_train, X_test, y_train, y_test)\n",
    "\n",
    "  results.append((k, accuracy))\n",
    "\n",
    "# Find the best k\n",
    "best_k, best_accuracy = max(results, key=lambda x: x[1])\n",
    "\n",
    "print(f\"Best k: {best_k}, Best Accuracy: {best_accuracy}\")\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------------------------\n",
      "Amount of features selected: 5\n",
      "Model Accuracy: 0.8591549295774648\n",
      "---------------------------------\n",
      "Amount of features selected: 6\n",
      "Model Accuracy: 0.8732394366197183\n",
      "---------------------------------\n",
      "Amount of features selected: 7\n",
      "Model Accuracy: 0.8873239436619719\n",
      "---------------------------------\n",
      "Amount of features selected: 8\n",
      "Model Accuracy: 0.8873239436619719\n",
      "---------------------------------\n",
      "Amount of features selected: 9\n",
      "Model Accuracy: 0.8873239436619719\n",
      "Best k: 7, Best Accuracy: 0.8873239436619719\n"
     ]
    }
   ],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply Mutual Information Classification (MIC)\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# Wrap mutual_info_classif with a fixed random_state\n",
    "mutual_info_classif_with_random_state = lambda X, y: mutual_info_classif(X, y, random_state=42)\n",
    "\n",
    "results = []\n",
    "for k in range(MIN_FEATURES, MAX_FEATURES):\n",
    "  # SelectKBest with mutual_info_classification evaluates all features\n",
    "  X_top = get_top_features_with_selector(\n",
    "      selector=SelectKBest(score_func=mutual_info_classif_with_random_state, k='all'),\n",
    "      num_of_features_to_select=k,\n",
    "      data_with_features=data_after_scaling,\n",
    "      target_variables=target_variables,\n",
    "      algorithm=\"Mutual Information Classification\",\n",
    "      verbose=True\n",
    "  )\n",
    "\n",
    "  # Split the data into train and test sets\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_top, target_variables, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Train and fit random forest classification model based on feature selected\n",
    "  accuracy = train_and_fit_random_forest(X_train, X_test, y_train, y_test)\n",
    "\n",
    "  results.append((k, accuracy))\n",
    "\n",
    "# Find the best k\n",
    "best_k, best_accuracy = max(results, key=lambda x: x[1])\n",
    "\n",
    "print(f\"Best k: {best_k}, Best Accuracy: {best_accuracy}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fUgKQV416YIo",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1737712620542,
     "user_tz": -120,
     "elapsed": 2191288,
     "user": {
      "displayName": "תומר וסרמן",
      "userId": "13570990899464465396"
     }
    },
    "outputId": "9977bd55-9f64-4c4d-9d92-ad498ebe6e02",
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:14.695454Z",
     "start_time": "2025-01-24T20:17:13.828724Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Rankings using Mutual Information Classification:\n",
      "        Feature     Score\n",
      "5    Attribute6  0.297178\n",
      "4    Attribute5  0.292443\n",
      "7    Attribute8  0.286131\n",
      "20  Attribute21  0.279286\n",
      "26  Attribute27  0.276162\n",
      "32  Attribute33  0.259260\n",
      "30  Attribute31  0.249639\n",
      "28  Attribute29  0.248604\n",
      "2    Attribute3  0.245162\n",
      "13  Attribute14  0.243970\n",
      "12  Attribute13  0.241300\n",
      "15  Attribute16  0.238697\n",
      "6    Attribute7  0.232773\n",
      "23  Attribute24  0.221345\n",
      "33  Attribute34  0.220364\n",
      "14  Attribute15  0.217917\n",
      "22  Attribute23  0.215758\n",
      "31  Attribute32  0.207568\n",
      "24  Attribute25  0.204661\n",
      "21  Attribute22  0.201930\n",
      "8    Attribute9  0.200808\n",
      "3    Attribute4  0.196269\n",
      "25  Attribute26  0.193824\n",
      "17  Attribute18  0.189906\n",
      "9   Attribute10  0.189552\n",
      "11  Attribute12  0.185869\n",
      "19  Attribute20  0.185076\n",
      "27  Attribute28  0.183910\n",
      "10  Attribute11  0.172547\n",
      "18  Attribute19  0.160522\n",
      "29  Attribute30  0.154623\n",
      "16  Attribute17  0.152221\n",
      "0    Attribute1  0.097375\n",
      "1    Attribute2  0.000000\n",
      "Model Accuracy: 0.9295774647887324\n",
      "Feature Rankings using Mutual Information Classification:\n",
      "        Feature     Score\n",
      "5    Attribute6  0.297178\n",
      "4    Attribute5  0.292443\n",
      "7    Attribute8  0.286131\n",
      "20  Attribute21  0.279286\n",
      "26  Attribute27  0.276162\n",
      "32  Attribute33  0.259260\n",
      "30  Attribute31  0.249639\n",
      "28  Attribute29  0.248604\n",
      "2    Attribute3  0.245162\n",
      "13  Attribute14  0.243970\n",
      "12  Attribute13  0.241300\n",
      "15  Attribute16  0.238697\n",
      "6    Attribute7  0.232773\n",
      "23  Attribute24  0.221345\n",
      "33  Attribute34  0.220364\n",
      "14  Attribute15  0.217917\n",
      "22  Attribute23  0.215758\n",
      "31  Attribute32  0.207568\n",
      "24  Attribute25  0.204661\n",
      "21  Attribute22  0.201930\n",
      "8    Attribute9  0.200808\n",
      "3    Attribute4  0.196269\n",
      "25  Attribute26  0.193824\n",
      "17  Attribute18  0.189906\n",
      "9   Attribute10  0.189552\n",
      "11  Attribute12  0.185869\n",
      "19  Attribute20  0.185076\n",
      "27  Attribute28  0.183910\n",
      "10  Attribute11  0.172547\n",
      "18  Attribute19  0.160522\n",
      "29  Attribute30  0.154623\n",
      "16  Attribute17  0.152221\n",
      "0    Attribute1  0.097375\n",
      "1    Attribute2  0.000000\n",
      "Model Accuracy: 0.9295774647887324\n",
      "Feature Rankings using Mutual Information Classification:\n",
      "        Feature     Score\n",
      "5    Attribute6  0.297178\n",
      "4    Attribute5  0.292443\n",
      "7    Attribute8  0.286131\n",
      "20  Attribute21  0.279286\n",
      "26  Attribute27  0.276162\n",
      "32  Attribute33  0.259260\n",
      "30  Attribute31  0.249639\n",
      "28  Attribute29  0.248604\n",
      "2    Attribute3  0.245162\n",
      "13  Attribute14  0.243970\n",
      "12  Attribute13  0.241300\n",
      "15  Attribute16  0.238697\n",
      "6    Attribute7  0.232773\n",
      "23  Attribute24  0.221345\n",
      "33  Attribute34  0.220364\n",
      "14  Attribute15  0.217917\n",
      "22  Attribute23  0.215758\n",
      "31  Attribute32  0.207568\n",
      "24  Attribute25  0.204661\n",
      "21  Attribute22  0.201930\n",
      "8    Attribute9  0.200808\n",
      "3    Attribute4  0.196269\n",
      "25  Attribute26  0.193824\n",
      "17  Attribute18  0.189906\n",
      "9   Attribute10  0.189552\n",
      "11  Attribute12  0.185869\n",
      "19  Attribute20  0.185076\n",
      "27  Attribute28  0.183910\n",
      "10  Attribute11  0.172547\n",
      "18  Attribute19  0.160522\n",
      "29  Attribute30  0.154623\n",
      "16  Attribute17  0.152221\n",
      "0    Attribute1  0.097375\n",
      "1    Attribute2  0.000000\n",
      "Model Accuracy: 0.9295774647887324\n",
      "Feature Rankings using Mutual Information Classification:\n",
      "        Feature     Score\n",
      "5    Attribute6  0.297178\n",
      "4    Attribute5  0.292443\n",
      "7    Attribute8  0.286131\n",
      "20  Attribute21  0.279286\n",
      "26  Attribute27  0.276162\n",
      "32  Attribute33  0.259260\n",
      "30  Attribute31  0.249639\n",
      "28  Attribute29  0.248604\n",
      "2    Attribute3  0.245162\n",
      "13  Attribute14  0.243970\n",
      "12  Attribute13  0.241300\n",
      "15  Attribute16  0.238697\n",
      "6    Attribute7  0.232773\n",
      "23  Attribute24  0.221345\n",
      "33  Attribute34  0.220364\n",
      "14  Attribute15  0.217917\n",
      "22  Attribute23  0.215758\n",
      "31  Attribute32  0.207568\n",
      "24  Attribute25  0.204661\n",
      "21  Attribute22  0.201930\n",
      "8    Attribute9  0.200808\n",
      "3    Attribute4  0.196269\n",
      "25  Attribute26  0.193824\n",
      "17  Attribute18  0.189906\n",
      "9   Attribute10  0.189552\n",
      "11  Attribute12  0.185869\n",
      "19  Attribute20  0.185076\n",
      "27  Attribute28  0.183910\n",
      "10  Attribute11  0.172547\n",
      "18  Attribute19  0.160522\n",
      "29  Attribute30  0.154623\n",
      "16  Attribute17  0.152221\n",
      "0    Attribute1  0.097375\n",
      "1    Attribute2  0.000000\n",
      "Model Accuracy: 0.9295774647887324\n",
      "Feature Rankings using Mutual Information Classification:\n",
      "        Feature     Score\n",
      "5    Attribute6  0.297178\n",
      "4    Attribute5  0.292443\n",
      "7    Attribute8  0.286131\n",
      "20  Attribute21  0.279286\n",
      "26  Attribute27  0.276162\n",
      "32  Attribute33  0.259260\n",
      "30  Attribute31  0.249639\n",
      "28  Attribute29  0.248604\n",
      "2    Attribute3  0.245162\n",
      "13  Attribute14  0.243970\n",
      "12  Attribute13  0.241300\n",
      "15  Attribute16  0.238697\n",
      "6    Attribute7  0.232773\n",
      "23  Attribute24  0.221345\n",
      "33  Attribute34  0.220364\n",
      "14  Attribute15  0.217917\n",
      "22  Attribute23  0.215758\n",
      "31  Attribute32  0.207568\n",
      "24  Attribute25  0.204661\n",
      "21  Attribute22  0.201930\n",
      "8    Attribute9  0.200808\n",
      "3    Attribute4  0.196269\n",
      "25  Attribute26  0.193824\n",
      "17  Attribute18  0.189906\n",
      "9   Attribute10  0.189552\n",
      "11  Attribute12  0.185869\n",
      "19  Attribute20  0.185076\n",
      "27  Attribute28  0.183910\n",
      "10  Attribute11  0.172547\n",
      "18  Attribute19  0.160522\n",
      "29  Attribute30  0.154623\n",
      "16  Attribute17  0.152221\n",
      "0    Attribute1  0.097375\n",
      "1    Attribute2  0.000000\n",
      "Model Accuracy: 0.9295774647887324\n",
      "Best k: 5, Best Accuracy: 0.9295774647887324\n"
     ]
    }
   ],
   "execution_count": 17
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply Mutual Information Regression (MIR)\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "\n",
    "results = []\n",
    "\n",
    "# Wrapping mutual_info_regression with a fixed random_state\n",
    "mutual_info_regression_with_random_state = lambda X, y: mutual_info_regression(X, y, random_state=42)\n",
    "\n",
    "for k in range(MIN_FEATURES, MAX_FEATURES):\n",
    "  # SelectKBest with mutual_info_regression evaluates all features\n",
    "  X_top = get_top_features_with_selector(\n",
    "      selector=SelectKBest(score_func=mutual_info_regression_with_random_state, k='all'),\n",
    "      num_of_features_to_select=k,\n",
    "      data_with_features=data_after_scaling,\n",
    "      target_variables=target_variables,\n",
    "      algorithm=\"Mutual Information Regression\",\n",
    "      verbose=True\n",
    "  )\n",
    "  # Split the data into train and test sets\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_top, target_variables, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Train and fit random forest classification model based on feature selected\n",
    "  accuracy = train_and_fit_random_forest(X_train, X_test, y_train, y_test)\n",
    "\n",
    "  results.append((k, accuracy))\n",
    "\n",
    "# Find the best k\n",
    "best_k, best_accuracy = max(results, key=lambda x: x[1])\n",
    "\n",
    "print(f\"Best k: {best_k}, Best Accuracy: {best_accuracy}\")"
   ],
   "metadata": {
    "id": "KvD-1ZX9Ixjf",
    "executionInfo": {
     "status": "aborted",
     "timestamp": 1737710403135,
     "user_tz": -120,
     "elapsed": 3,
     "user": {
      "displayName": "תומר וסרמן",
      "userId": "13570990899464465396"
     }
    },
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:15.635484Z",
     "start_time": "2025-01-24T20:17:14.739186Z"
    }
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "could not convert string to float: 'g'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[18], line 11\u001B[0m\n\u001B[0;32m      7\u001B[0m mutual_info_regression_with_random_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m X, y: mutual_info_regression(X, y, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(MIN_FEATURES, MAX_FEATURES):\n\u001B[0;32m     10\u001B[0m   \u001B[38;5;66;03m# SelectKBest with mutual_info_regression evaluates all features\u001B[39;00m\n\u001B[1;32m---> 11\u001B[0m   X_top \u001B[38;5;241m=\u001B[39m get_top_features_with_selector(\n\u001B[0;32m     12\u001B[0m       selector\u001B[38;5;241m=\u001B[39mSelectKBest(score_func\u001B[38;5;241m=\u001B[39mmutual_info_regression_with_random_state, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[0;32m     13\u001B[0m       num_of_features_to_select\u001B[38;5;241m=\u001B[39mk,\n\u001B[0;32m     14\u001B[0m       data_with_features\u001B[38;5;241m=\u001B[39mdata_after_scaling,\n\u001B[0;32m     15\u001B[0m       target_variables\u001B[38;5;241m=\u001B[39mtarget_variables,\n\u001B[0;32m     16\u001B[0m       algorithm\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mMutual Information Regression\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     17\u001B[0m       verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     18\u001B[0m   )\n\u001B[0;32m     19\u001B[0m   \u001B[38;5;66;03m# Split the data into train and test sets\u001B[39;00m\n\u001B[0;32m     20\u001B[0m   X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(X_top, target_variables, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n",
      "Cell \u001B[1;32mIn[14], line 17\u001B[0m, in \u001B[0;36mget_top_features_with_selector\u001B[1;34m(selector, num_of_features_to_select, data_with_features, target_variables, algorithm, verbose)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_top_features_with_selector\u001B[39m(selector: SelectKBest,\n\u001B[0;32m      2\u001B[0m                                     num_of_features_to_select: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m      3\u001B[0m                                     data_with_features,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      6\u001B[0m                                     verbose: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m      7\u001B[0m                                     ):\n\u001B[0;32m      8\u001B[0m \u001B[38;5;250m      \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;124;03m      :param selector: SelectKBest object.\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;124;03m      :param num_of_features_to_select:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m      :return:\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;124;03m      \"\"\"\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m       selector\u001B[38;5;241m.\u001B[39mfit(data_with_features, target_variables)\n\u001B[0;32m     19\u001B[0m       \u001B[38;5;66;03m# Rank the features using Chi-Square algorithm\u001B[39;00m\n\u001B[0;32m     20\u001B[0m       top_features \u001B[38;5;241m=\u001B[39m get_top_k_features(selector\u001B[38;5;241m=\u001B[39mselector, feature_names\u001B[38;5;241m=\u001B[39mfeatures\u001B[38;5;241m.\u001B[39mcolumns,\n\u001B[0;32m     21\u001B[0m                                         top_features_to_select\u001B[38;5;241m=\u001B[39mnum_of_features_to_select, algorithm\u001B[38;5;241m=\u001B[39malgorithm,\n\u001B[0;32m     22\u001B[0m                                         verbose\u001B[38;5;241m=\u001B[39mverbose)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:567\u001B[0m, in \u001B[0;36m_BaseFilter.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    562\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_data(\n\u001B[0;32m    563\u001B[0m         X, y, accept_sparse\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsc\u001B[39m\u001B[38;5;124m\"\u001B[39m], multi_output\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    564\u001B[0m     )\n\u001B[0;32m    566\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params(X, y)\n\u001B[1;32m--> 567\u001B[0m score_func_ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscore_func(X, y)\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(score_func_ret, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscores_, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpvalues_ \u001B[38;5;241m=\u001B[39m score_func_ret\n",
      "Cell \u001B[1;32mIn[18], line 7\u001B[0m, in \u001B[0;36m<lambda>\u001B[1;34m(X, y)\u001B[0m\n\u001B[0;32m      4\u001B[0m results \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      6\u001B[0m \u001B[38;5;66;03m# Wrapping mutual_info_regression with a fixed random_state\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m mutual_info_regression_with_random_state \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mlambda\u001B[39;00m X, y: mutual_info_regression(X, y, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n\u001B[0;32m      9\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(MIN_FEATURES, MAX_FEATURES):\n\u001B[0;32m     10\u001B[0m   \u001B[38;5;66;03m# SelectKBest with mutual_info_regression evaluates all features\u001B[39;00m\n\u001B[0;32m     11\u001B[0m   X_top \u001B[38;5;241m=\u001B[39m get_top_features_with_selector(\n\u001B[0;32m     12\u001B[0m       selector\u001B[38;5;241m=\u001B[39mSelectKBest(score_func\u001B[38;5;241m=\u001B[39mmutual_info_regression_with_random_state, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[0;32m     13\u001B[0m       num_of_features_to_select\u001B[38;5;241m=\u001B[39mk,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     17\u001B[0m       verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     18\u001B[0m   )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    184\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[1;32m--> 186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    188\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[0;32m    190\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:441\u001B[0m, in \u001B[0;36mmutual_info_regression\u001B[1;34m(X, y, discrete_features, n_neighbors, copy, random_state, n_jobs)\u001B[0m\n\u001B[0;32m    325\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[0;32m    326\u001B[0m     {\n\u001B[0;32m    327\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray-like\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    345\u001B[0m     n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    346\u001B[0m ):\n\u001B[0;32m    347\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Estimate mutual information for a continuous target variable.\u001B[39;00m\n\u001B[0;32m    348\u001B[0m \n\u001B[0;32m    349\u001B[0m \u001B[38;5;124;03m    Mutual information (MI) [1]_ between two random variables is a non-negative\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    439\u001B[0m \u001B[38;5;124;03m    array([0.1..., 2.6...  , 0.0...])\u001B[39;00m\n\u001B[0;32m    440\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 441\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m _estimate_mi(\n\u001B[0;32m    442\u001B[0m         X,\n\u001B[0;32m    443\u001B[0m         y,\n\u001B[0;32m    444\u001B[0m         discrete_features\u001B[38;5;241m=\u001B[39mdiscrete_features,\n\u001B[0;32m    445\u001B[0m         discrete_target\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mFalse\u001B[39;00m,\n\u001B[0;32m    446\u001B[0m         n_neighbors\u001B[38;5;241m=\u001B[39mn_neighbors,\n\u001B[0;32m    447\u001B[0m         copy\u001B[38;5;241m=\u001B[39mcopy,\n\u001B[0;32m    448\u001B[0m         random_state\u001B[38;5;241m=\u001B[39mrandom_state,\n\u001B[0;32m    449\u001B[0m         n_jobs\u001B[38;5;241m=\u001B[39mn_jobs,\n\u001B[0;32m    450\u001B[0m     )\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_mutual_info.py:271\u001B[0m, in \u001B[0;36m_estimate_mi\u001B[1;34m(X, y, discrete_features, discrete_target, n_neighbors, copy, random_state, n_jobs)\u001B[0m\n\u001B[0;32m    202\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21m_estimate_mi\u001B[39m(\n\u001B[0;32m    203\u001B[0m     X,\n\u001B[0;32m    204\u001B[0m     y,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    211\u001B[0m     n_jobs\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mNone\u001B[39;00m,\n\u001B[0;32m    212\u001B[0m ):\n\u001B[0;32m    213\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Estimate mutual information between the features and the target.\u001B[39;00m\n\u001B[0;32m    214\u001B[0m \n\u001B[0;32m    215\u001B[0m \u001B[38;5;124;03m    Parameters\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    269\u001B[0m \u001B[38;5;124;03m           Data Sets\". PLoS ONE 9(2), 2014.\u001B[39;00m\n\u001B[0;32m    270\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 271\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m check_X_y(X, y, accept_sparse\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsc\u001B[39m\u001B[38;5;124m\"\u001B[39m, y_numeric\u001B[38;5;241m=\u001B[39m\u001B[38;5;129;01mnot\u001B[39;00m discrete_target)\n\u001B[0;32m    272\u001B[0m     n_samples, n_features \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mshape\n\u001B[0;32m    274\u001B[0m     \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(discrete_features, (\u001B[38;5;28mstr\u001B[39m, \u001B[38;5;28mbool\u001B[39m)):\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1318\u001B[0m, in \u001B[0;36mcheck_X_y\u001B[1;34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1297\u001B[0m     \u001B[38;5;28;01mraise\u001B[39;00m \u001B[38;5;167;01mValueError\u001B[39;00m(\n\u001B[0;32m   1298\u001B[0m         \u001B[38;5;124mf\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;132;01m{\u001B[39;00mestimator_name\u001B[38;5;132;01m}\u001B[39;00m\u001B[38;5;124m requires y to be passed, but the target y is None\u001B[39m\u001B[38;5;124m\"\u001B[39m\n\u001B[0;32m   1299\u001B[0m     )\n\u001B[0;32m   1301\u001B[0m X \u001B[38;5;241m=\u001B[39m check_array(\n\u001B[0;32m   1302\u001B[0m     X,\n\u001B[0;32m   1303\u001B[0m     accept_sparse\u001B[38;5;241m=\u001B[39maccept_sparse,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m   1315\u001B[0m     input_name\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m   1316\u001B[0m )\n\u001B[1;32m-> 1318\u001B[0m y \u001B[38;5;241m=\u001B[39m _check_y(y, multi_output\u001B[38;5;241m=\u001B[39mmulti_output, y_numeric\u001B[38;5;241m=\u001B[39my_numeric, estimator\u001B[38;5;241m=\u001B[39mestimator)\n\u001B[0;32m   1320\u001B[0m check_consistent_length(X, y)\n\u001B[0;32m   1322\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m X, y\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\validation.py:1343\u001B[0m, in \u001B[0;36m_check_y\u001B[1;34m(y, multi_output, y_numeric, estimator)\u001B[0m\n\u001B[0;32m   1341\u001B[0m     _ensure_no_complex_data(y)\n\u001B[0;32m   1342\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m y_numeric \u001B[38;5;129;01mand\u001B[39;00m \u001B[38;5;28mhasattr\u001B[39m(y\u001B[38;5;241m.\u001B[39mdtype, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mkind\u001B[39m\u001B[38;5;124m\"\u001B[39m) \u001B[38;5;129;01mand\u001B[39;00m y\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mkind \u001B[38;5;241m==\u001B[39m \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mO\u001B[39m\u001B[38;5;124m\"\u001B[39m:\n\u001B[1;32m-> 1343\u001B[0m     y \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39mastype(np\u001B[38;5;241m.\u001B[39mfloat64)\n\u001B[0;32m   1345\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m y\n",
      "\u001B[1;31mValueError\u001B[0m: could not convert string to float: 'g'"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply ANOVA F-value Classification\n",
    "from sklearn.feature_selection import f_classif\n",
    "\n",
    "results = []\n",
    "for k in range(MIN_FEATURES, MAX_FEATURES):\n",
    "  # SelectKBest with f_classif evaluates all features\n",
    "  X_top = get_top_features_with_selector(\n",
    "      selector=SelectKBest(score_func=f_classif, k='all'),\n",
    "      num_of_features_to_select=k,\n",
    "      data_with_features=data_after_scaling,\n",
    "      target_variables=target_variables,\n",
    "      algorithm=\"ANOVA F-value Classificaiton\",\n",
    "      verbose=True\n",
    "  )\n",
    "  # Split the data into train and test sets\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_top, target_variables, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Train and fit random forest classification model based on feature selected\n",
    "  accuracy = train_and_fit_random_forest(X_train, X_test, y_train, y_test)\n",
    "\n",
    "  results.append((k, accuracy))\n",
    "\n",
    "# Find the best k\n",
    "best_k, best_accuracy = max(results, key=lambda x: x[1])\n",
    "\n",
    "print(f\"Best k: {best_k}, Best Accuracy: {best_accuracy}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "O9tPxGglO_KN",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1737712839445,
     "user_tz": -120,
     "elapsed": 202652,
     "user": {
      "displayName": "תומר וסרמן",
      "userId": "13570990899464465396"
     }
    },
    "outputId": "80c0c6a7-d3d8-403f-a172-e0ed17948f48",
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:36.250068Z",
     "start_time": "2025-01-24T20:17:35.678568Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aviv Metz\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [1] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\Aviv Metz\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "C:\\Users\\Aviv Metz\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [1] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\Aviv Metz\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Rankings using ANOVA F-value Classificaiton:\n",
      "        Feature       Score\n",
      "2    Attribute3  128.762319\n",
      "4    Attribute5  126.961973\n",
      "0    Attribute1   96.605478\n",
      "6    Attribute7   88.829431\n",
      "8    Attribute9   33.230308\n",
      "30  Attribute31   33.122984\n",
      "32  Attribute33   25.545122\n",
      "28  Attribute29   23.273858\n",
      "20  Attribute21   17.680082\n",
      "7    Attribute8   15.709751\n",
      "14  Attribute15   15.655475\n",
      "22  Attribute23   15.210672\n",
      "13  Attribute14   14.097289\n",
      "24  Attribute25   12.813044\n",
      "12  Attribute13   11.913104\n",
      "10  Attribute11   10.124789\n",
      "11  Attribute12    9.162092\n",
      "5    Attribute6    7.934810\n",
      "15  Attribute16    7.899602\n",
      "3    Attribute4    5.619616\n",
      "9   Attribute10    5.153808\n",
      "17  Attribute18    5.042790\n",
      "18  Attribute19    4.880385\n",
      "21  Attribute22    4.792263\n",
      "26  Attribute27    4.362180\n",
      "16  Attribute17    2.665416\n",
      "33  Attribute34    1.442961\n",
      "27  Attribute28    0.639170\n",
      "31  Attribute32    0.452990\n",
      "19  Attribute20    0.443373\n",
      "23  Attribute24    0.013387\n",
      "29  Attribute30    0.005422\n",
      "25  Attribute26    0.000829\n",
      "1    Attribute2         NaN\n",
      "Model Accuracy: 0.8873239436619719\n",
      "Feature Rankings using ANOVA F-value Classificaiton:\n",
      "        Feature       Score\n",
      "2    Attribute3  128.762319\n",
      "4    Attribute5  126.961973\n",
      "0    Attribute1   96.605478\n",
      "6    Attribute7   88.829431\n",
      "8    Attribute9   33.230308\n",
      "30  Attribute31   33.122984\n",
      "32  Attribute33   25.545122\n",
      "28  Attribute29   23.273858\n",
      "20  Attribute21   17.680082\n",
      "7    Attribute8   15.709751\n",
      "14  Attribute15   15.655475\n",
      "22  Attribute23   15.210672\n",
      "13  Attribute14   14.097289\n",
      "24  Attribute25   12.813044\n",
      "12  Attribute13   11.913104\n",
      "10  Attribute11   10.124789\n",
      "11  Attribute12    9.162092\n",
      "5    Attribute6    7.934810\n",
      "15  Attribute16    7.899602\n",
      "3    Attribute4    5.619616\n",
      "9   Attribute10    5.153808\n",
      "17  Attribute18    5.042790\n",
      "18  Attribute19    4.880385\n",
      "21  Attribute22    4.792263\n",
      "26  Attribute27    4.362180\n",
      "16  Attribute17    2.665416\n",
      "33  Attribute34    1.442961\n",
      "27  Attribute28    0.639170\n",
      "31  Attribute32    0.452990\n",
      "19  Attribute20    0.443373\n",
      "23  Attribute24    0.013387\n",
      "29  Attribute30    0.005422\n",
      "25  Attribute26    0.000829\n",
      "1    Attribute2         NaN\n",
      "Model Accuracy: 0.8873239436619719\n",
      "Feature Rankings using ANOVA F-value Classificaiton:\n",
      "        Feature       Score\n",
      "2    Attribute3  128.762319\n",
      "4    Attribute5  126.961973\n",
      "0    Attribute1   96.605478\n",
      "6    Attribute7   88.829431\n",
      "8    Attribute9   33.230308\n",
      "30  Attribute31   33.122984\n",
      "32  Attribute33   25.545122\n",
      "28  Attribute29   23.273858\n",
      "20  Attribute21   17.680082\n",
      "7    Attribute8   15.709751\n",
      "14  Attribute15   15.655475\n",
      "22  Attribute23   15.210672\n",
      "13  Attribute14   14.097289\n",
      "24  Attribute25   12.813044\n",
      "12  Attribute13   11.913104\n",
      "10  Attribute11   10.124789\n",
      "11  Attribute12    9.162092\n",
      "5    Attribute6    7.934810\n",
      "15  Attribute16    7.899602\n",
      "3    Attribute4    5.619616\n",
      "9   Attribute10    5.153808\n",
      "17  Attribute18    5.042790\n",
      "18  Attribute19    4.880385\n",
      "21  Attribute22    4.792263\n",
      "26  Attribute27    4.362180\n",
      "16  Attribute17    2.665416\n",
      "33  Attribute34    1.442961\n",
      "27  Attribute28    0.639170\n",
      "31  Attribute32    0.452990\n",
      "19  Attribute20    0.443373\n",
      "23  Attribute24    0.013387\n",
      "29  Attribute30    0.005422\n",
      "25  Attribute26    0.000829\n",
      "1    Attribute2         NaN\n",
      "Model Accuracy: 0.9014084507042254\n",
      "Feature Rankings using ANOVA F-value Classificaiton:\n",
      "        Feature       Score\n",
      "2    Attribute3  128.762319\n",
      "4    Attribute5  126.961973\n",
      "0    Attribute1   96.605478\n",
      "6    Attribute7   88.829431\n",
      "8    Attribute9   33.230308\n",
      "30  Attribute31   33.122984\n",
      "32  Attribute33   25.545122\n",
      "28  Attribute29   23.273858\n",
      "20  Attribute21   17.680082\n",
      "7    Attribute8   15.709751\n",
      "14  Attribute15   15.655475\n",
      "22  Attribute23   15.210672\n",
      "13  Attribute14   14.097289\n",
      "24  Attribute25   12.813044\n",
      "12  Attribute13   11.913104\n",
      "10  Attribute11   10.124789\n",
      "11  Attribute12    9.162092\n",
      "5    Attribute6    7.934810\n",
      "15  Attribute16    7.899602\n",
      "3    Attribute4    5.619616\n",
      "9   Attribute10    5.153808\n",
      "17  Attribute18    5.042790\n",
      "18  Attribute19    4.880385\n",
      "21  Attribute22    4.792263\n",
      "26  Attribute27    4.362180\n",
      "16  Attribute17    2.665416\n",
      "33  Attribute34    1.442961\n",
      "27  Attribute28    0.639170\n",
      "31  Attribute32    0.452990\n",
      "19  Attribute20    0.443373\n",
      "23  Attribute24    0.013387\n",
      "29  Attribute30    0.005422\n",
      "25  Attribute26    0.000829\n",
      "1    Attribute2         NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aviv Metz\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [1] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\Aviv Metz\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n",
      "C:\\Users\\Aviv Metz\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [1] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\Aviv Metz\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model Accuracy: 0.8873239436619719\n",
      "Feature Rankings using ANOVA F-value Classificaiton:\n",
      "        Feature       Score\n",
      "2    Attribute3  128.762319\n",
      "4    Attribute5  126.961973\n",
      "0    Attribute1   96.605478\n",
      "6    Attribute7   88.829431\n",
      "8    Attribute9   33.230308\n",
      "30  Attribute31   33.122984\n",
      "32  Attribute33   25.545122\n",
      "28  Attribute29   23.273858\n",
      "20  Attribute21   17.680082\n",
      "7    Attribute8   15.709751\n",
      "14  Attribute15   15.655475\n",
      "22  Attribute23   15.210672\n",
      "13  Attribute14   14.097289\n",
      "24  Attribute25   12.813044\n",
      "12  Attribute13   11.913104\n",
      "10  Attribute11   10.124789\n",
      "11  Attribute12    9.162092\n",
      "5    Attribute6    7.934810\n",
      "15  Attribute16    7.899602\n",
      "3    Attribute4    5.619616\n",
      "9   Attribute10    5.153808\n",
      "17  Attribute18    5.042790\n",
      "18  Attribute19    4.880385\n",
      "21  Attribute22    4.792263\n",
      "26  Attribute27    4.362180\n",
      "16  Attribute17    2.665416\n",
      "33  Attribute34    1.442961\n",
      "27  Attribute28    0.639170\n",
      "31  Attribute32    0.452990\n",
      "19  Attribute20    0.443373\n",
      "23  Attribute24    0.013387\n",
      "29  Attribute30    0.005422\n",
      "25  Attribute26    0.000829\n",
      "1    Attribute2         NaN\n",
      "Model Accuracy: 0.8732394366197183\n",
      "Best k: 7, Best Accuracy: 0.9014084507042254\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Aviv Metz\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:112: UserWarning: Features [1] are constant.\n",
      "  warnings.warn(\"Features %s are constant.\" % constant_features_idx, UserWarning)\n",
      "C:\\Users\\Aviv Metz\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:113: RuntimeWarning: invalid value encountered in divide\n",
      "  f = msb / msw\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "cell_type": "code",
   "source": [
    "# Apply ANOVA F-value Regression\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "results = []\n",
    "for k in range(MIN_FEATURES, MAX_FEATURES):\n",
    "  # SelectKBest with f_regression evaluates all features\n",
    "  X_top = get_top_features_with_selector(\n",
    "      selector=SelectKBest(score_func=f_regression, k='all'),\n",
    "      num_of_features_to_select=k,\n",
    "      data_with_features=data_after_scaling,\n",
    "      target_variables=target_variables,\n",
    "      algorithm=\"ANOVA F-value Regression\",\n",
    "      verbose=True\n",
    "  )\n",
    "  # Split the data into train and test sets\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_top, target_variables, test_size=0.2, random_state=42)\n",
    "\n",
    "  # Train and fit random forest classification model based on feature selected\n",
    "  accuracy = train_and_fit_random_forest(X_train, X_test, y_train, y_test)\n",
    "  results.append((k, accuracy))\n",
    "\n",
    "# Find the best k\n",
    "best_k, best_accuracy = max(results, key=lambda x: x[1])\n",
    "\n",
    "print(f\"Best k: {best_k}, Best Accuracy: {best_accuracy}\")"
   ],
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6g17o-1HQmBp",
    "executionInfo": {
     "status": "ok",
     "timestamp": 1737713873092,
     "user_tz": -120,
     "elapsed": 270905,
     "user": {
      "displayName": "תומר וסרמן",
      "userId": "13570990899464465396"
     }
    },
    "outputId": "dc0d89ca-e9fe-4a0a-e8c9-a9276fd2651a",
    "ExecuteTime": {
     "end_time": "2025-01-24T20:17:41.356821Z",
     "start_time": "2025-01-24T20:17:41.101447Z"
    }
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "ufunc 'divide' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mTypeError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[20], line 7\u001B[0m\n\u001B[0;32m      4\u001B[0m results \u001B[38;5;241m=\u001B[39m []\n\u001B[0;32m      5\u001B[0m \u001B[38;5;28;01mfor\u001B[39;00m k \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mrange\u001B[39m(MIN_FEATURES, MAX_FEATURES):\n\u001B[0;32m      6\u001B[0m   \u001B[38;5;66;03m# SelectKBest with f_regression evaluates all features\u001B[39;00m\n\u001B[1;32m----> 7\u001B[0m   X_top \u001B[38;5;241m=\u001B[39m get_top_features_with_selector(\n\u001B[0;32m      8\u001B[0m       selector\u001B[38;5;241m=\u001B[39mSelectKBest(score_func\u001B[38;5;241m=\u001B[39mf_regression, k\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m'\u001B[39m\u001B[38;5;124mall\u001B[39m\u001B[38;5;124m'\u001B[39m),\n\u001B[0;32m      9\u001B[0m       num_of_features_to_select\u001B[38;5;241m=\u001B[39mk,\n\u001B[0;32m     10\u001B[0m       data_with_features\u001B[38;5;241m=\u001B[39mdata_after_scaling,\n\u001B[0;32m     11\u001B[0m       target_variables\u001B[38;5;241m=\u001B[39mtarget_variables,\n\u001B[0;32m     12\u001B[0m       algorithm\u001B[38;5;241m=\u001B[39m\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mANOVA F-value Regression\u001B[39m\u001B[38;5;124m\"\u001B[39m,\n\u001B[0;32m     13\u001B[0m       verbose\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m     14\u001B[0m   )\n\u001B[0;32m     15\u001B[0m   \u001B[38;5;66;03m# Split the data into train and test sets\u001B[39;00m\n\u001B[0;32m     16\u001B[0m   X_train, X_test, y_train, y_test \u001B[38;5;241m=\u001B[39m train_test_split(X_top, target_variables, test_size\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0.2\u001B[39m, random_state\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m42\u001B[39m)\n",
      "Cell \u001B[1;32mIn[14], line 17\u001B[0m, in \u001B[0;36mget_top_features_with_selector\u001B[1;34m(selector, num_of_features_to_select, data_with_features, target_variables, algorithm, verbose)\u001B[0m\n\u001B[0;32m      1\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mget_top_features_with_selector\u001B[39m(selector: SelectKBest,\n\u001B[0;32m      2\u001B[0m                                     num_of_features_to_select: \u001B[38;5;28mint\u001B[39m,\n\u001B[0;32m      3\u001B[0m                                     data_with_features,\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m      6\u001B[0m                                     verbose: \u001B[38;5;28mbool\u001B[39m \u001B[38;5;241m=\u001B[39m \u001B[38;5;28;01mFalse\u001B[39;00m\n\u001B[0;32m      7\u001B[0m                                     ):\n\u001B[0;32m      8\u001B[0m \u001B[38;5;250m      \u001B[39m\u001B[38;5;124;03m\"\"\"\u001B[39;00m\n\u001B[0;32m      9\u001B[0m \u001B[38;5;124;03m      :param selector: SelectKBest object.\u001B[39;00m\n\u001B[0;32m     10\u001B[0m \u001B[38;5;124;03m      :param num_of_features_to_select:\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m     15\u001B[0m \u001B[38;5;124;03m      :return:\u001B[39;00m\n\u001B[0;32m     16\u001B[0m \u001B[38;5;124;03m      \"\"\"\u001B[39;00m\n\u001B[1;32m---> 17\u001B[0m       selector\u001B[38;5;241m.\u001B[39mfit(data_with_features, target_variables)\n\u001B[0;32m     19\u001B[0m       \u001B[38;5;66;03m# Rank the features using Chi-Square algorithm\u001B[39;00m\n\u001B[0;32m     20\u001B[0m       top_features \u001B[38;5;241m=\u001B[39m get_top_k_features(selector\u001B[38;5;241m=\u001B[39mselector, feature_names\u001B[38;5;241m=\u001B[39mfeatures\u001B[38;5;241m.\u001B[39mcolumns,\n\u001B[0;32m     21\u001B[0m                                         top_features_to_select\u001B[38;5;241m=\u001B[39mnum_of_features_to_select, algorithm\u001B[38;5;241m=\u001B[39malgorithm,\n\u001B[0;32m     22\u001B[0m                                         verbose\u001B[38;5;241m=\u001B[39mverbose)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\base.py:1473\u001B[0m, in \u001B[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001B[1;34m(estimator, *args, **kwargs)\u001B[0m\n\u001B[0;32m   1466\u001B[0m     estimator\u001B[38;5;241m.\u001B[39m_validate_params()\n\u001B[0;32m   1468\u001B[0m \u001B[38;5;28;01mwith\u001B[39;00m config_context(\n\u001B[0;32m   1469\u001B[0m     skip_parameter_validation\u001B[38;5;241m=\u001B[39m(\n\u001B[0;32m   1470\u001B[0m         prefer_skip_nested_validation \u001B[38;5;129;01mor\u001B[39;00m global_skip_validation\n\u001B[0;32m   1471\u001B[0m     )\n\u001B[0;32m   1472\u001B[0m ):\n\u001B[1;32m-> 1473\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m fit_method(estimator, \u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:567\u001B[0m, in \u001B[0;36m_BaseFilter.fit\u001B[1;34m(self, X, y)\u001B[0m\n\u001B[0;32m    562\u001B[0m     X, y \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_validate_data(\n\u001B[0;32m    563\u001B[0m         X, y, accept_sparse\u001B[38;5;241m=\u001B[39m[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsr\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mcsc\u001B[39m\u001B[38;5;124m\"\u001B[39m], multi_output\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m\n\u001B[0;32m    564\u001B[0m     )\n\u001B[0;32m    566\u001B[0m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39m_check_params(X, y)\n\u001B[1;32m--> 567\u001B[0m score_func_ret \u001B[38;5;241m=\u001B[39m \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscore_func(X, y)\n\u001B[0;32m    568\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;28misinstance\u001B[39m(score_func_ret, (\u001B[38;5;28mlist\u001B[39m, \u001B[38;5;28mtuple\u001B[39m)):\n\u001B[0;32m    569\u001B[0m     \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mscores_, \u001B[38;5;28mself\u001B[39m\u001B[38;5;241m.\u001B[39mpvalues_ \u001B[38;5;241m=\u001B[39m score_func_ret\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    184\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[1;32m--> 186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    188\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[0;32m    190\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:498\u001B[0m, in \u001B[0;36mf_regression\u001B[1;34m(X, y, center, force_finite)\u001B[0m\n\u001B[0;32m    396\u001B[0m \u001B[38;5;129m@validate_params\u001B[39m(\n\u001B[0;32m    397\u001B[0m     {\n\u001B[0;32m    398\u001B[0m         \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mX\u001B[39m\u001B[38;5;124m\"\u001B[39m: [\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124marray-like\u001B[39m\u001B[38;5;124m\"\u001B[39m, \u001B[38;5;124m\"\u001B[39m\u001B[38;5;124msparse matrix\u001B[39m\u001B[38;5;124m\"\u001B[39m],\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    404\u001B[0m )\n\u001B[0;32m    405\u001B[0m \u001B[38;5;28;01mdef\u001B[39;00m \u001B[38;5;21mf_regression\u001B[39m(X, y, \u001B[38;5;241m*\u001B[39m, center\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m, force_finite\u001B[38;5;241m=\u001B[39m\u001B[38;5;28;01mTrue\u001B[39;00m):\n\u001B[0;32m    406\u001B[0m \u001B[38;5;250m    \u001B[39m\u001B[38;5;124;03m\"\"\"Univariate linear regression tests returning F-statistic and p-values.\u001B[39;00m\n\u001B[0;32m    407\u001B[0m \n\u001B[0;32m    408\u001B[0m \u001B[38;5;124;03m    Quick linear model for testing the effect of a single regressor,\u001B[39;00m\n\u001B[1;32m   (...)\u001B[0m\n\u001B[0;32m    496\u001B[0m \u001B[38;5;124;03m    array([2.7..., 1.5..., 1.0...])\u001B[39;00m\n\u001B[0;32m    497\u001B[0m \u001B[38;5;124;03m    \"\"\"\u001B[39;00m\n\u001B[1;32m--> 498\u001B[0m     correlation_coefficient \u001B[38;5;241m=\u001B[39m r_regression(\n\u001B[0;32m    499\u001B[0m         X, y, center\u001B[38;5;241m=\u001B[39mcenter, force_finite\u001B[38;5;241m=\u001B[39mforce_finite\n\u001B[0;32m    500\u001B[0m     )\n\u001B[0;32m    501\u001B[0m     deg_of_freedom \u001B[38;5;241m=\u001B[39m y\u001B[38;5;241m.\u001B[39msize \u001B[38;5;241m-\u001B[39m (\u001B[38;5;241m2\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m center \u001B[38;5;28;01melse\u001B[39;00m \u001B[38;5;241m1\u001B[39m)\n\u001B[0;32m    503\u001B[0m     corr_coef_squared \u001B[38;5;241m=\u001B[39m correlation_coefficient\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39m\u001B[38;5;241m2\u001B[39m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:186\u001B[0m, in \u001B[0;36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001B[1;34m(*args, **kwargs)\u001B[0m\n\u001B[0;32m    184\u001B[0m global_skip_validation \u001B[38;5;241m=\u001B[39m get_config()[\u001B[38;5;124m\"\u001B[39m\u001B[38;5;124mskip_parameter_validation\u001B[39m\u001B[38;5;124m\"\u001B[39m]\n\u001B[0;32m    185\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m global_skip_validation:\n\u001B[1;32m--> 186\u001B[0m     \u001B[38;5;28;01mreturn\u001B[39;00m func(\u001B[38;5;241m*\u001B[39margs, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[0;32m    188\u001B[0m func_sig \u001B[38;5;241m=\u001B[39m signature(func)\n\u001B[0;32m    190\u001B[0m \u001B[38;5;66;03m# Map *args/**kwargs to the function signature\u001B[39;00m\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\sklearn\\feature_selection\\_univariate_selection.py:370\u001B[0m, in \u001B[0;36mr_regression\u001B[1;34m(X, y, center, force_finite)\u001B[0m\n\u001B[0;32m    366\u001B[0m \u001B[38;5;66;03m# Compute centered values\u001B[39;00m\n\u001B[0;32m    367\u001B[0m \u001B[38;5;66;03m# Note that E[(x - mean(x))*(y - mean(y))] = E[x*(y - mean(y))], so we\u001B[39;00m\n\u001B[0;32m    368\u001B[0m \u001B[38;5;66;03m# need not center X\u001B[39;00m\n\u001B[0;32m    369\u001B[0m \u001B[38;5;28;01mif\u001B[39;00m center:\n\u001B[1;32m--> 370\u001B[0m     y \u001B[38;5;241m=\u001B[39m y \u001B[38;5;241m-\u001B[39m np\u001B[38;5;241m.\u001B[39mmean(y)\n\u001B[0;32m    371\u001B[0m     \u001B[38;5;66;03m# TODO: for Scipy <= 1.10, `isspmatrix(X)` returns `True` for sparse arrays.\u001B[39;00m\n\u001B[0;32m    372\u001B[0m     \u001B[38;5;66;03m# Here, we check the output of the `.mean` operation that returns a `np.matrix`\u001B[39;00m\n\u001B[0;32m    373\u001B[0m     \u001B[38;5;66;03m# for sparse matrices while a `np.array` for dense and sparse arrays.\u001B[39;00m\n\u001B[0;32m    374\u001B[0m     \u001B[38;5;66;03m# We can reconsider using `isspmatrix` when the minimum version is\u001B[39;00m\n\u001B[0;32m    375\u001B[0m     \u001B[38;5;66;03m# SciPy >= 1.11\u001B[39;00m\n\u001B[0;32m    376\u001B[0m     X_means \u001B[38;5;241m=\u001B[39m X\u001B[38;5;241m.\u001B[39mmean(axis\u001B[38;5;241m=\u001B[39m\u001B[38;5;241m0\u001B[39m)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\numpy\\_core\\fromnumeric.py:3860\u001B[0m, in \u001B[0;36mmean\u001B[1;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[0;32m   3857\u001B[0m     \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[0;32m   3858\u001B[0m         \u001B[38;5;28;01mreturn\u001B[39;00m mean(axis\u001B[38;5;241m=\u001B[39maxis, dtype\u001B[38;5;241m=\u001B[39mdtype, out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n\u001B[1;32m-> 3860\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m _methods\u001B[38;5;241m.\u001B[39m_mean(a, axis\u001B[38;5;241m=\u001B[39maxis, dtype\u001B[38;5;241m=\u001B[39mdtype,\n\u001B[0;32m   3861\u001B[0m                       out\u001B[38;5;241m=\u001B[39mout, \u001B[38;5;241m*\u001B[39m\u001B[38;5;241m*\u001B[39mkwargs)\n",
      "File \u001B[1;32m~\\miniconda3\\Lib\\site-packages\\numpy\\_core\\_methods.py:147\u001B[0m, in \u001B[0;36m_mean\u001B[1;34m(a, axis, dtype, out, keepdims, where)\u001B[0m\n\u001B[0;32m    145\u001B[0m         ret \u001B[38;5;241m=\u001B[39m ret\u001B[38;5;241m.\u001B[39mdtype\u001B[38;5;241m.\u001B[39mtype(ret \u001B[38;5;241m/\u001B[39m rcount)\n\u001B[0;32m    146\u001B[0m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[1;32m--> 147\u001B[0m     ret \u001B[38;5;241m=\u001B[39m ret \u001B[38;5;241m/\u001B[39m rcount\n\u001B[0;32m    149\u001B[0m \u001B[38;5;28;01mreturn\u001B[39;00m ret\n",
      "\u001B[1;31mTypeError\u001B[0m: ufunc 'divide' not supported for the input types, and the inputs could not be safely coerced to any supported types according to the casting rule ''safe''"
     ]
    }
   ],
   "execution_count": 20
  }
 ]
}
