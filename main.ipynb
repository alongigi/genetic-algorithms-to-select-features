{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!pip install ucimlrepo\n",
    "!pip install pygad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bec33f7cae768a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygad\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectKBest\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775d6e9179d6b50f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Indexes of relevant databases.\n",
    "ISOLET_DB_INDEX: int = 54\n",
    "SPAMBASE_DB_INDEX: int = 94"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b5a14b277f5a9c1",
   "metadata": {},
   "source": [
    "Change the following to true for much more robust error messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef2652aa59d70ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "VERBOSE: bool = False # For more robust error messages"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "427827ac8230572d",
   "metadata": {},
   "source": [
    "Importing and handling the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3653a86502cc81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read dataset\n",
    "dataset_currently_used = fetch_ucirepo(id=ISOLET_DB_INDEX)\n",
    "\n",
    "features = dataset_currently_used.data.features\n",
    "target_variables = dataset_currently_used.data.targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d33b31bad0421c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print missing values and maximum and minimum values in the features of the first dataset\n",
    "X_df = pd.DataFrame(features)\n",
    "y_df = pd.DataFrame(target_variables)\n",
    "\n",
    "print(\"Missing values in X:\", X_df.isnull().sum().sum())\n",
    "print(\"Missing values in y:\", y_df.isnull().sum().sum())\n",
    "\n",
    "print(\"Minimum value across all features:\", X_df.min().min())\n",
    "print(\"Maximum value across all features:\", X_df.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64313dee7d6054df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize the dataset\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_df)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_df.columns)\n",
    "\n",
    "print(X_scaled_df.min().min())\n",
    "print(X_scaled_df.max().max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88c0991ca7f0b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_after_scaling = X_scaled_df\n",
    "target_variables = y_df.values.ravel()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4121de36c96f5cba",
   "metadata": {},
   "source": [
    "Feature selection functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641eabc4d83c5a34",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_k_features(selector,\n",
    "                       feature_names: list,\n",
    "                       top_features_to_select: int,\n",
    "                       algorithm: str,\n",
    "                       verbose: bool = False,\n",
    "                       normalize_score: bool = True):\n",
    "  \"\"\"\n",
    "  Get the top k features based on their scores from a SelectKBest selector.\n",
    "\n",
    "  Parameters:\n",
    "  selector (SelectKBest): Fitted SelectKBest object.\n",
    "  feature_names (list): List of feature names (columns of X).\n",
    "  k (int): Number of top features to select.\n",
    "  algorithm (str): The name of the feature selection algorithm.\n",
    "  verbose (bool): Decide whether to print model results or not.\n",
    "  normalize_score (bool): Decide whether to normalize the score or not.\n",
    "\n",
    "\n",
    "  Returns:\n",
    "  A dataframe that contains 2 columns: The first is \"Feature\" and is the feature name and the second is a score, normalization is dependent on the var sent..\n",
    "\n",
    "  \"\"\"\n",
    "  # Retrieve feature scores\n",
    "  scores = selector.scores_\n",
    "\n",
    "  if normalize_score:\n",
    "      scores = scores / np.nansum(scores)\n",
    "\n",
    "  feature_ranking = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Score': scores\n",
    "  }).sort_values(by='Score', ascending=False)\n",
    "  if verbose:\n",
    "    # Display top-ranked features\n",
    "    print(f\"Feature Rankings using {algorithm}:\")\n",
    "    print(feature_ranking)\n",
    "\n",
    "\n",
    "  # Return selected top k features\n",
    "  return feature_ranking.head(top_features_to_select)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0071fc97b791c87",
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_FOREST_SEED: int = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8a8d5a8331edcf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_fit_random_forest(X_train, X_test, y_train, y_test, verbose: bool = VERBOSE):\n",
    "  \"\"\"\n",
    "  Builds, trains, and evaluates a Random Forest classification model.\n",
    "\n",
    "  Parameters:\n",
    "  ----------\n",
    "  X_train : pd.DataFrame or np.ndarray\n",
    "      Feature matrix for training the model.\n",
    "  X_test : pd.DataFrame or np.ndarray\n",
    "      Feature matrix for testing the model.\n",
    "  y_train : pd.Series or np.ndarray\n",
    "      Target labels for training the model.\n",
    "  y_test : pd.Series or np.ndarray\n",
    "      True target labels for testing the model.\n",
    "  verbose (bool): Decide whether to print model results or not\n",
    "\n",
    "  Returns:\n",
    "  float: The accuracy of the model on the selected features\n",
    "  \"\"\"\n",
    "  # Build a simple classification model\n",
    "  model = RandomForestClassifier(random_state=RANDOM_FOREST_SEED)\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Evaluate the model\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  if verbose:\n",
    "    print(\"Model Accuracy:\", accuracy)\n",
    "    # Detailed performance metrics\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "  # Return the accuracy of the model\n",
    "  return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60656a67680f31c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "MIN_FEATURES: int = 5\n",
    "MAX_FEATURES: int = 10\n",
    "TRAIN_TEST_SPLIT_RATIO: float = 0.2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fedf31d48d2d5d3d",
   "metadata": {},
   "source": [
    "Import all the feature selection algorithms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99638d58bd84024",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "mutual_info_classif_with_random_state = lambda X, y: mutual_info_classif(X, y, random_state=42)\n",
    "mutual_info_regression_with_random_state = lambda X, y: mutual_info_regression(X, y, random_state=42)\n",
    "\n",
    "# Selects features based on the k best scores. Here k is 'all'.\n",
    "classifier_chi2: SelectKBest = SelectKBest(score_func=chi2, k='all')\n",
    "classifier_mutual_info_classif: SelectKBest = SelectKBest(score_func=mutual_info_classif_with_random_state, k='all')\n",
    "classifier_mutual_info_regression: SelectKBest = SelectKBest(score_func=mutual_info_regression_with_random_state, k='all')\n",
    "classifier_f_classif: SelectKBest = SelectKBest(score_func=f_classif, k='all')\n",
    "classifier_f_regression: SelectKBest = SelectKBest(score_func=f_regression, k='all')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a1efd3ac68487d7",
   "metadata": {},
   "source": [
    "Preselecting all the features with each classifier to get a feature ranking."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9715f9685b56b95c",
   "metadata": {},
   "outputs": [],
   "source": [
    "FEATURES_TO_SELECT = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0b36a01d9e3141e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fits all the feature selection algorithms.\n",
    "\n",
    "selector_list = [\n",
    "    (classifier_chi2, \"classifier_chi2\"),\n",
    "    (classifier_mutual_info_classif, \"classifier_mutual_info_classif\"),\n",
    "    (classifier_mutual_info_regression, \"classifier_mutual_info_regression\"),\n",
    "    (classifier_f_classif, \"classifier_f_classif\"),\n",
    "    (classifier_f_regression, \"classifier_f_regression\")\n",
    "]\n",
    "features_selected_by_each_algorithm: dict = dict()\n",
    "for selector_in_list in selector_list:\n",
    "    selector_in_list: tuple[SelectKBest, str]\n",
    "    selector_in_list[0].fit(data_after_scaling, target_variables)\n",
    "      # Rank the features using Chi-Square algorithm\n",
    "    top_features = get_top_k_features(\n",
    "        selector=selector_in_list[0],\n",
    "        feature_names=features.columns,\n",
    "        top_features_to_select=FEATURES_TO_SELECT,\n",
    "        algorithm=selector_in_list[1],\n",
    "    )\n",
    "    features_selected_by_each_algorithm[selector_in_list[1]] = top_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf04ba6ab50bc8cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_combine_feature_scores(features_selected_by_each_algorithm_in_func: dict[str, pd.DataFrame],\n",
    "                                    weights: list[float],\n",
    "                                    verbose: bool = VERBOSE) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Sums all the features selected by each algorithm, multiplying each feature by the weight corresponding to it's index.\n",
    "    :param features_selected_by_each_algorithm_in_func:\n",
    "    :param weights:\n",
    "    :param verbose: Printing relevant messages.\n",
    "    :return:\n",
    "    \"\"\"\n",
    "    if len(features_selected_by_each_algorithm_in_func) != len(weights):\n",
    "        raise ValueError(\"Number of weights does not match number of features selectors\")\n",
    "    combined_scores = pd.DataFrame({\n",
    "        'Feature': [],\n",
    "        'Score': []\n",
    "    })\n",
    "    index = 0\n",
    "    for algorithm_name, features_selected_by_algorithm in features_selected_by_each_algorithm_in_func.items():\n",
    "        if verbose:\n",
    "            print(f'Combining scores for {algorithm_name}, its weight is: {weights[index]}')\n",
    "\n",
    "        features_selected_by_algorithm['Score'] = features_selected_by_algorithm['Score'] * weights[index]\n",
    "        combined_scores = pd.merge(combined_scores,\n",
    "                               features_selected_by_algorithm,\n",
    "                               on='Feature', how='outer', suffixes=('_df1', '_df2'))\n",
    "\n",
    "        # Sum the scores where both exist, fill NaN with 0 for features that only exist in one of the dataframes\n",
    "        combined_scores['Score'] = combined_scores['Score_df1'].fillna(0) + combined_scores['Score_df2'].fillna(0)\n",
    "\n",
    "        # Drop the original score columns if not needed\n",
    "        combined_scores = combined_scores[['Feature', 'Score']]\n",
    "        index += 1\n",
    "    return combined_scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41fd9161dd569135",
   "metadata": {},
   "source": [
    "Genetic Algorithm part, will use the cells created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b0f42e782a6e8cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_TRAIN_SPLIT = 0.2\n",
    "RANDOM_STATE_OF_DATA_SPLIT = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f35fa2e9e7ed25af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Genetic Algorithm parameters\n",
    "NUMBER_OF_GENERATIONS = 1\n",
    "NUMBER_OF_PARENTS_MATING = 4\n",
    "\n",
    "SOLUTIONS_PER_POPULATION = 12\n",
    "NUMBER_OF_GENES = len(features_selected_by_each_algorithm) # Use this to control the number of feature selection potential solutions is used.\n",
    "\n",
    "INIT_RANGE_LOW = 0\n",
    "INIT_RANGE_HIGH = 1\n",
    "\n",
    "PARENT_SELECTION_TYPE = \"sss\" #steady-state selection, meaning it selects the parents with the highest fitness.\n",
    "KEEP_PARENTS = 1\n",
    "\n",
    "CROSSOVER_TYPE = \"single_point\" # Swaps the chromosomes from a certain index onwards between the parents.\n",
    "\n",
    "MUTATION_TYPE = \"random\"\n",
    "MUTATION_PERCENT_GENES = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f58d7337b2f9fe12",
   "metadata": {},
   "source": [
    "Running the genetic algorithm, using the other feature selection methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4110f85d8de9d9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_func_as_weights_to_use_from_each_algorithm(ga_instance, solution, solution_idx):\n",
    "    combined_scores = weighted_combine_feature_scores(features_selected_by_each_algorithm_in_func=features_selected_by_each_algorithm,\n",
    "                                                      weights=solution)\n",
    "    combined_sorted_scores = combined_scores.sort_values(by=['Score'], ascending=False)\n",
    "    list_of_sorted_features = combined_sorted_scores.head(FEATURES_TO_SELECT)['Feature'].tolist()\n",
    "    data_with_top_features = data_after_scaling[list_of_sorted_features]\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_with_top_features, target_variables, test_size=TEST_TRAIN_SPLIT, random_state=RANDOM_STATE_OF_DATA_SPLIT)\n",
    "    # Train and fit random forest classification model based on feature selected\n",
    "    accuracy = train_and_fit_random_forest(X_train, X_test, y_train, y_test)\n",
    "    if VERBOSE:\n",
    "        print(f'model_accuracy: {accuracy}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e69ec6e51c9fa814",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "ga_instance_feature_selection_algorithms = (\n",
    "    pygad.GA(num_generations=NUMBER_OF_GENERATIONS,\n",
    "             num_parents_mating=NUMBER_OF_PARENTS_MATING,  # Num of parents to select each generation.\n",
    "             fitness_func=fitness_func_as_weights_to_use_from_each_algorithm,\n",
    "             sol_per_pop=SOLUTIONS_PER_POPULATION,  # Number of solutions per population.\n",
    "             num_genes=NUMBER_OF_GENES,  # Effectively, the thing that is tweaked for each generation.\n",
    "             # gene_type=list[float], # The type of gene, meaning of each value inside a chromosome. Supports list.\n",
    "             init_range_low=INIT_RANGE_LOW,  # dependent on the gene type, the range of values to be generated.\n",
    "             init_range_high=INIT_RANGE_HIGH,\n",
    "             parent_selection_type=PARENT_SELECTION_TYPE,\n",
    "             keep_parents=KEEP_PARENTS,  # Number of parents to keep from current population.\n",
    "             # keep_elitism = 1, # The number of the solutions with the best fitness that will be kept for next generation.\n",
    "             crossover_type=CROSSOVER_TYPE,\n",
    "             mutation_type=MUTATION_TYPE,\n",
    "             mutation_by_replacement=True,  # If the previous gene is replaced or not.\n",
    "             mutation_percent_genes=MUTATION_PERCENT_GENES,  # The probability that each gene will be mutated\n",
    "             # crossover_type=crossover_func, Can be used to customize a crossover func.\n",
    "             # mutation_type=mutation_func, Can be used to customize a mutation func.\n",
    "             )\n",
    ")\n",
    "\n",
    "ga_instance_feature_selection_algorithms.run()\n",
    "print('Running feature selection based on other feature selection algorithms')\n",
    "print('--------------------------------------------------')\n",
    "print(f'Generation: {NUMBER_OF_GENERATIONS}')\n",
    "solution, solution_fitness, solution_idx = ga_instance_feature_selection_algorithms.best_solution()\n",
    "print(\"Parameters of the best solution : {solution}\".format(solution=solution))\n",
    "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5964c00c30e91b5",
   "metadata": {},
   "source": [
    "Running the genetic algorithm (With the same parameters as the previous genetic algorithm) but selecting the features directly.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b5a92a372240533",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fitness_func_on_features_themselves(ga_instance, solution, solution_idx):\n",
    "    solution: list[np.float64]\n",
    "\n",
    "    raise NotImplementedError(\"Please implement selecting a feature based on the solution array\")\n",
    "    data_after_selecting_features =\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_with_specific_features, target_variables, test_size=TEST_TRAIN_SPLIT, random_state=RANDOM_STATE_OF_DATA_SPLIT)\n",
    "    # Train and fit random forest classification model based on feature selected\n",
    "    accuracy = train_and_fit_random_forest(X_train, X_test, y_train, y_test)\n",
    "    if VERBOSE:\n",
    "        print(f'model_accuracy: {accuracy}')\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4af71898f6d7bf3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "raise NotImplementedError(\"Add the number of possible features here.\")\n",
    "NUMBER_OF_POSSIBLE_FEATURES: int = 123 # <--- Add info here.\n",
    "GENE_VALUES: list = [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e4d0dbc1cd8d461",
   "metadata": {},
   "outputs": [],
   "source": [
    "gene_space = [GENE_VALUES for _ in range(NUMBER_OF_POSSIBLE_FEATURES)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c751836ff8f2b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "ga_instance_selecting_features_directly = (\n",
    "    pygad.GA(num_generations=NUMBER_OF_GENERATIONS,\n",
    "             num_parents_mating=NUMBER_OF_PARENTS_MATING,  # Num of parents to select each generation.\n",
    "             fitness_func=fitness_func_on_features_themselves,\n",
    "             sol_per_pop=SOLUTIONS_PER_POPULATION,  # Number of solutions per population.\n",
    "             num_genes=NUMBER_OF_POSSIBLE_FEATURES,  # Effectively, the thing that is tweaked for each generation.\n",
    "             # gene_type=list[float], # The type of gene, meaning of each value inside a chromosome. Supports list.\n",
    "             init_range_low=INIT_RANGE_LOW,  # dependent on the gene type, the range of values to be generated.\n",
    "             init_range_high=INIT_RANGE_HIGH,\n",
    "             parent_selection_type=PARENT_SELECTION_TYPE,\n",
    "             keep_parents=KEEP_PARENTS,  # Number of parents to keep from current population.\n",
    "             # keep_elitism = 1, # The number of the solutions with the best fitness that will be kept for next generation.\n",
    "             crossover_type=CROSSOVER_TYPE,\n",
    "             mutation_type=MUTATION_TYPE,\n",
    "             mutation_by_replacement=True,  # If the previous gene is replaced or not.\n",
    "             mutation_percent_genes=MUTATION_PERCENT_GENES,  # The probability that each gene will be mutated\n",
    "             # crossover_type=crossover_func, Can be used to customize a crossover func.\n",
    "             # mutation_type=mutation_func, Can be used to customize a mutation func.\n",
    "             gene_space=gene_space\n",
    "             )\n",
    ")\n",
    "\n",
    "ga_instance_selecting_features_directly.run()\n",
    "print('Running feature selection based on simple genetic algorithm')\n",
    "print('--------------------------------------------------')\n",
    "print(f'Generation: {NUMBER_OF_GENERATIONS}')\n",
    "solution, solution_fitness, solution_idx = ga_instance_selecting_features_directly.best_solution()\n",
    "print(\"Parameters of the best solution : {solution}\".format(solution=solution))\n",
    "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf23b80a10461249",
   "metadata": {},
   "source": [
    "Training the model with the features selected by each algorithm individually to achieve ablation study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52e546e8f8eb92b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Checking the performance of the features selected by each model independently.\n",
    "\n",
    "for feature_selection_method_name, features_selected_with_score in features_selected_by_each_algorithm.items():\n",
    "    features_selected_by_algorithm = features_selected_with_score.head(FEATURES_TO_SELECT)['Feature'].tolist()\n",
    "    data_afer_selecting_features = data_after_scaling[features_selected_by_algorithm]\n",
    "    X_train, X_test, y_train, y_test = train_test_split(data_afer_selecting_features, target_variables, test_size=TEST_TRAIN_SPLIT, random_state=42)\n",
    "    accuracy = train_and_fit_random_forest(X_train, X_test, y_train, y_test)\n",
    "    print('------------------------------------------------')\n",
    "    print(f'Feature selection method: {feature_selection_method_name}. Accuracy: {accuracy}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
