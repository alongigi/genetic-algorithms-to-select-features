{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "!pip install ucimlrepo\n",
    "!pip install pygad"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygad\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectKBest\n"
   ],
   "id": "2bec33f7cae768a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ISOLET_DB_INDEX: int = 54\n",
    "SPAMBASE_DB_INDEX: int = 94"
   ],
   "id": "775d6e9179d6b50f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read first dataset\n",
    "isolet = fetch_ucirepo(id=52) # Using this one cause it works.\n",
    "\n",
    "features = isolet.data.features\n",
    "target_variables = isolet.data.targets"
   ],
   "id": "8e3653a86502cc81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print missing values and maximum and minimum values in the features of the first dataset\n",
    "X_df = pd.DataFrame(features)\n",
    "y_df = pd.DataFrame(target_variables)\n",
    "\n",
    "print(\"Missing values in X:\", X_df.isnull().sum().sum())\n",
    "print(\"Missing values in y:\", y_df.isnull().sum().sum())\n",
    "\n",
    "print(\"Minimum value across all features:\", X_df.min().min())\n",
    "print(\"Maximum value across all features:\", X_df.max().max())"
   ],
   "id": "5d33b31bad0421c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Normalize the first dataset\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_df)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_df.columns)\n",
    "\n",
    "print(X_scaled_df.min().min())\n",
    "print(X_scaled_df.max().max())"
   ],
   "id": "64313dee7d6054df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_after_scaling = X_scaled_df\n",
    "target_variables = y_df.values.ravel()"
   ],
   "id": "88c0991ca7f0b398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:05:52.618657Z",
     "start_time": "2025-01-24T21:05:52.614369Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_top_k_features(selector,\n",
    "                       feature_names: list,\n",
    "                       top_features_to_select: int,\n",
    "                       algorithm: str,\n",
    "                       verbose: bool = False,\n",
    "                       normalize_score: bool = True):\n",
    "  \"\"\"\n",
    "  Get the top k features based on their scores from a SelectKBest selector.\n",
    "\n",
    "  Parameters:\n",
    "  selector (SelectKBest): Fitted SelectKBest object.\n",
    "  feature_names (list): List of feature names (columns of X).\n",
    "  k (int): Number of top features to select.\n",
    "  algorithm (str): The name of the feature selection algorithm.\n",
    "\n",
    "  Returns:\n",
    "  list: Names of the top k features.\n",
    "  \"\"\"\n",
    "  # Retrieve feature scores\n",
    "  scores = selector.scores_\n",
    "\n",
    "  if normalize_score:\n",
    "      scores = scores / np.nansum(scores)\n",
    "\n",
    "  feature_ranking = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Score': scores\n",
    "  }).sort_values(by='Score', ascending=False)\n",
    "  if verbose:\n",
    "    # Display top-ranked features\n",
    "    print(f\"Feature Rankings using {algorithm}:\")\n",
    "    print(feature_ranking)\n",
    "\n",
    "\n",
    "  # Return selected top k features\n",
    "  return feature_ranking.head(top_features_to_select)['Feature'].tolist()"
   ],
   "id": "641eabc4d83c5a34",
   "outputs": [],
   "execution_count": 60
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:00:13.370217Z",
     "start_time": "2025-01-24T21:00:13.367832Z"
    }
   },
   "cell_type": "code",
   "source": "RANDOM_FOREST_SEED: int = 42",
   "id": "d0071fc97b791c87",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:00:13.427062Z",
     "start_time": "2025-01-24T21:00:13.424071Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def train_and_fit_random_forest(X_train, X_test, y_train, y_test):\n",
    "  \"\"\"\n",
    "  Builds, trains, and evaluates a Random Forest classification model.\n",
    "\n",
    "  Parameters:\n",
    "  ----------\n",
    "  X_train : pd.DataFrame or np.ndarray\n",
    "      Feature matrix for training the model.\n",
    "  X_test : pd.DataFrame or np.ndarray\n",
    "      Feature matrix for testing the model.\n",
    "  y_train : pd.Series or np.ndarray\n",
    "      Target labels for training the model.\n",
    "  y_test : pd.Series or np.ndarray\n",
    "      True target labels for testing the model.\n",
    "\n",
    "  Returns:\n",
    "  float: The accuracy of the model on the selected features\n",
    "  \"\"\"\n",
    "  # Build a simple classification model\n",
    "  model = RandomForestClassifier(random_state=RANDOM_FOREST_SEED)\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Evaluate the model\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "  # Detailed performance metrics\n",
    "  # print(\"\\nClassification Report:\")\n",
    "  # print(classification_report(y_test, y_pred))\n",
    "\n",
    "  # Return the accuracy of the model\n",
    "  return accuracy\n"
   ],
   "id": "b8a8d5a8331edcf9",
   "outputs": [],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:00:13.434687Z",
     "start_time": "2025-01-24T21:00:13.431035Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_top_features_with_selector(selector: SelectKBest,\n",
    "                                    num_of_features_to_select: int,\n",
    "                                    data_with_features,\n",
    "                                    target_variables,\n",
    "                                    algorithm: str = \"\",\n",
    "                                    verbose: bool = False\n",
    "                                    ):\n",
    "      \"\"\"\n",
    "      :param selector: SelectKBest object.\n",
    "      :param num_of_features_to_select:\n",
    "      :param data_with_features: The features are selected from this data.\n",
    "      :param target_variables: The variable the feature selection is used on.\n",
    "      :param algorithm: The algorithm used, as a str. Used for debug printouts.\n",
    "      :param verbose: Enable debug printouts.\n",
    "      :return:\n",
    "      \"\"\"\n",
    "      selector.fit(data_with_features, target_variables)\n",
    "\n",
    "      # Rank the features using Chi-Square algorithm\n",
    "      top_features = get_top_k_features(selector=selector, feature_names=features.columns,\n",
    "                                        top_features_to_select=num_of_features_to_select, algorithm=algorithm,\n",
    "                                        verbose=verbose)\n",
    "\n",
    "      return data_with_features[top_features]"
   ],
   "id": "7dfb35ab03fc1301",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:00:13.486805Z",
     "start_time": "2025-01-24T21:00:13.483622Z"
    }
   },
   "cell_type": "code",
   "source": [
    "MIN_FEATURES: int = 5\n",
    "MAX_FEATURES: int = 10\n",
    "TRAIN_TEST_SPLIT_RATIO: float = 0.2\n"
   ],
   "id": "60656a67680f31c3",
   "outputs": [],
   "execution_count": 47
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import all the feature selection algorithms.",
   "id": "fedf31d48d2d5d3d"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:00:13.539827Z",
     "start_time": "2025-01-24T21:00:13.536164Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "mutual_info_classif_with_random_state = lambda X, y: mutual_info_classif(X, y, random_state=42)\n",
    "mutual_info_regression_with_random_state = lambda X, y: mutual_info_regression(X, y, random_state=42)\n",
    "\n",
    "# Selects featuers based on the k best scores. Here k is 'all'.\n",
    "classifier_chi2: SelectKBest = SelectKBest(score_func=chi2, k='all')\n",
    "classifier_mutual_info_classif: SelectKBest = SelectKBest(score_func=mutual_info_classif_with_random_state, k='all')\n",
    "classifier_mutual_info_regression: SelectKBest = SelectKBest(score_func=mutual_info_regression_with_random_state, k='all')\n",
    "classifier_f_classif: SelectKBest = SelectKBest(score_func=f_classif, k='all')\n",
    "classifier_f_regression: SelectKBest = SelectKBest(score_func=f_regression, k='all')"
   ],
   "id": "99638d58bd84024",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preselecting all the features with each classifier to get a feature ranking.",
   "id": "2a1efd3ac68487d7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:00:13.589632Z",
     "start_time": "2025-01-24T21:00:13.587461Z"
    }
   },
   "cell_type": "code",
   "source": "FEATURES_TO_SELECT = 5",
   "id": "9715f9685b56b95c",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:05:57.146990Z",
     "start_time": "2025-01-24T21:05:57.074334Z"
    }
   },
   "cell_type": "code",
   "source": [
    "selector_list = [\n",
    "    (classifier_chi2, \"classifier_chi2\"),\n",
    "    (classifier_mutual_info_classif, \"classifier_mutual_info_classif\"),\n",
    "    # (classifier_mutual_info_regression, \"classifier_mutual_info_regression\"),\n",
    "    # (classifier_f_classif, \"classifier_f_classif\"),\n",
    "    # (classifier_f_regression, \"classifier_f_regression\")\n",
    "]\n",
    "features_selected_by_each_algorithm: dict = dict()\n",
    "for selector_in_list in selector_list:\n",
    "    selector_in_list: tuple[SelectKBest, str]\n",
    "    selector_in_list[0].fit(data_after_scaling, target_variables)\n",
    "      # Rank the features using Chi-Square algorithm\n",
    "    top_features = get_top_k_features(\n",
    "        selector=selector_in_list[0],\n",
    "        feature_names=features.columns,\n",
    "        top_features_to_select=FEATURES_TO_SELECT,\n",
    "        algorithm=selector_in_list[1],\n",
    "        verbose=True\n",
    "    )\n",
    "    features_selected_by_each_algorithm[selector_in_list[1]] = top_features\n"
   ],
   "id": "f0b36a01d9e3141e",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SCORES SUM TO: 60.8102230477591\n",
      "SCORES AFTER NORMALIZATION TO: 0.9999999999999999\n",
      "Feature Rankings using classifier_chi2:\n",
      "        Feature     Score\n",
      "0    Attribute1  0.135475\n",
      "4    Attribute5  0.129577\n",
      "2    Attribute3  0.117055\n",
      "6    Attribute7  0.091419\n",
      "30  Attribute31  0.060236\n",
      "28  Attribute29  0.043286\n",
      "8    Attribute9  0.042549\n",
      "32  Attribute33  0.039736\n",
      "14  Attribute15  0.039173\n",
      "20  Attribute21  0.038605\n",
      "22  Attribute23  0.032156\n",
      "7    Attribute8  0.030031\n",
      "12  Attribute13  0.026251\n",
      "13  Attribute14  0.025025\n",
      "24  Attribute25  0.024425\n",
      "10  Attribute11  0.017452\n",
      "11  Attribute12  0.015605\n",
      "15  Attribute16  0.012494\n",
      "5    Attribute6  0.012174\n",
      "18  Attribute19  0.011451\n",
      "21  Attribute22  0.010380\n",
      "17  Attribute18  0.010152\n",
      "3    Attribute4  0.008509\n",
      "9   Attribute10  0.008299\n",
      "26  Attribute27  0.006141\n",
      "16  Attribute17  0.006029\n",
      "33  Attribute34  0.002562\n",
      "27  Attribute28  0.001711\n",
      "19  Attribute20  0.001008\n",
      "31  Attribute32  0.000988\n",
      "23  Attribute24  0.000033\n",
      "29  Attribute30  0.000012\n",
      "25  Attribute26  0.000002\n",
      "1    Attribute2       NaN\n",
      "SCORES SUM TO: 7.142649362544557\n",
      "SCORES AFTER NORMALIZATION TO: 1.0000000000000002\n",
      "Feature Rankings using classifier_mutual_info_classif:\n",
      "        Feature     Score\n",
      "5    Attribute6  0.041606\n",
      "4    Attribute5  0.040943\n",
      "7    Attribute8  0.040060\n",
      "20  Attribute21  0.039101\n",
      "26  Attribute27  0.038664\n",
      "32  Attribute33  0.036298\n",
      "30  Attribute31  0.034950\n",
      "28  Attribute29  0.034806\n",
      "2    Attribute3  0.034324\n",
      "13  Attribute14  0.034157\n",
      "12  Attribute13  0.033783\n",
      "15  Attribute16  0.033418\n",
      "6    Attribute7  0.032589\n",
      "23  Attribute24  0.030989\n",
      "33  Attribute34  0.030852\n",
      "14  Attribute15  0.030509\n",
      "22  Attribute23  0.030207\n",
      "31  Attribute32  0.029060\n",
      "24  Attribute25  0.028653\n",
      "21  Attribute22  0.028271\n",
      "8    Attribute9  0.028114\n",
      "3    Attribute4  0.027478\n",
      "25  Attribute26  0.027136\n",
      "17  Attribute18  0.026588\n",
      "9   Attribute10  0.026538\n",
      "11  Attribute12  0.026022\n",
      "19  Attribute20  0.025911\n",
      "27  Attribute28  0.025748\n",
      "10  Attribute11  0.024157\n",
      "18  Attribute19  0.022474\n",
      "29  Attribute30  0.021648\n",
      "16  Attribute17  0.021312\n",
      "0    Attribute1  0.013633\n",
      "1    Attribute2  0.000000\n"
     ]
    }
   ],
   "execution_count": 61
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:00:13.767779Z",
     "start_time": "2025-01-24T21:00:13.763561Z"
    }
   },
   "cell_type": "code",
   "source": [
    "function_weights = np.array([-4, 5, 7, 2.3, -2, 9, 9, -2, 5, 12, 57, 42, 42, 1, -3])\n",
    "desired_output = 100\n",
    "epsilon = 0.00001 # To prevent division by 0 errors.\n",
    "potential_solutions = np.random.randint(-10, 10, (len(function_weights),len(function_weights)))\n",
    "potential_solutions"
   ],
   "id": "8777a85c0b112906",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  8,  -1,  -9,  -9,  -9,  -7,  -4,  -8,  -7,   6,   3,   9,  -1,\n",
       "          5,  -7],\n",
       "       [ -9,  -9,   8,  -7,  -9,   8,   2,  -6,  -2,  -2,   8,  -2,   9,\n",
       "          8,   4],\n",
       "       [  7,   5,   2,   3,   5,   3,   4,  -7,   1,   1,   1,   0,   3,\n",
       "          0,   4],\n",
       "       [ -8,  -2,  -1,   6,   8,   3, -10,   1,  -4,   7,   1, -10,  -9,\n",
       "          4,  -6],\n",
       "       [ -1,  -6,   3,  -5,  -5,   2,  -7,  -5,  -2,  -9,   7,   7,  -8,\n",
       "         -1,   7],\n",
       "       [ -5,   7,  -2,   1,   1,   9,  -8,   8,   3,  -1,   0,   6,  -7,\n",
       "          7,   0],\n",
       "       [  3,   4,   5,   3,   8,  -4,  -2,   7,  -5,  -6,   0,   0,   5,\n",
       "          1,  -5],\n",
       "       [  9,   8,  -6,   3,   8,   4,  -6,  -9,   0,   2, -10,   1,   0,\n",
       "          9,   6],\n",
       "       [  1,   0,   6,   8,  -5,   0,   5,  -1,  -5,  -1,  -3,   1,  -6,\n",
       "         -1,   1],\n",
       "       [ -7,  -1,  -8,   8,   6,   7, -10,  -1,   3,  -2,  -2,   1,  -9,\n",
       "         -4,   5],\n",
       "       [  7,   3,   3,  -6,  -7,   1,  -2,  -6,   4,  -5,  -6,  -5,  -5,\n",
       "         -5,  -5],\n",
       "       [ -5, -10,   0,  -1,   7,  -3,   7,   7,   0,  -5,   9,   6,   1,\n",
       "         -4,   8],\n",
       "       [ -2,   6,  -2,  -5,  -6,  -1, -10, -10,   5,  -7,  -8,   4,  -7,\n",
       "        -10,  -9],\n",
       "       [ -8, -10,   4,  -7,   1,   5,   0,  -4,   6,  -4,   5,  -4,   0,\n",
       "          5,  -4],\n",
       "       [  3,  -5,  -2,   5,  -8,   8,   4, -10,  -5,  -4,   0,   1,   5,\n",
       "          2,   4]], dtype=int32)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:00:13.844070Z",
     "start_time": "2025-01-24T21:00:13.840299Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fitness_func_arr_of_weights(ga_instance, solution, solution_idx):\n",
    "    # Coded as an array of weights.\n",
    "    # TODO Here we can insert the feature selection and train the model, then classify, to measure fitness.\n",
    "    # TODO The higher fitness, the better.\n",
    "    summed_rows = np.sum(potential_solutions, axis=1)\n",
    "    solution_to_check = np.multiply(summed_rows, solution)\n",
    "    output = np.sum(solution_to_check * function_weights)\n",
    "    fitness = 1.0 / (np.abs(output - desired_output) + epsilon) #\n",
    "    return fitness\n"
   ],
   "id": "e03f8e73ff4e6507",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:00:13.887873Z",
     "start_time": "2025-01-24T21:00:13.885490Z"
    }
   },
   "cell_type": "code",
   "source": "#TODO Remove this afterwards.\n",
   "id": "e681a41f5f7f1a19",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:00:13.928268Z",
     "start_time": "2025-01-24T21:00:13.924771Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def fitness_func_as_weights_to_use_from_each_algorithm(ga_instance, solution, solution_idx):\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_top, target_variables, test_size=0.2, random_state=42)\n",
    "  # Train and fit random forest classification model based on feature selected\n",
    "  accuracy = train_and_fit_random_forest(X_train, X_test, y_train, y_test)"
   ],
   "id": "9701027316f75369",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:00:14.033880Z",
     "start_time": "2025-01-24T21:00:14.026899Z"
    }
   },
   "cell_type": "code",
   "source": [
    "num_generations = 1\n",
    "num_parents_mating = 4\n",
    "\n",
    "sol_per_pop = 12\n",
    "num_genes = function_weights.size # Use this to control the number of feature selection potential solutions is used.\n",
    "\n",
    "init_range_low = 0\n",
    "init_range_high = 10\n",
    "\n",
    "parent_selection_type = \"sss\" #steady-state selection, meaning it selects the parents with the highest fitness.\n",
    "keep_parents = 1\n",
    "\n",
    "crossover_type = \"single_point\" # Swaps the chromosomes from a certain index onwards between the parents.\n",
    "\n",
    "mutation_type = \"random\"\n",
    "mutation_percent_genes = 20\n",
    "ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                       num_parents_mating=num_parents_mating, # Num of parents to select each generation.\n",
    "                       fitness_func=fitness_func_arr_of_weights,\n",
    "                       sol_per_pop=sol_per_pop, # Number of solutions per population.\n",
    "                       num_genes=num_genes, # Effectively, the thing that is tweaked for each generation.\n",
    "                       # gene_type=list[float], # The type of gene, meaning of each value inside a chromosome. Supports list.\n",
    "                       init_range_low=init_range_low, # dependent on the gene type, the range of values to be generated.\n",
    "                       init_range_high=init_range_high,\n",
    "                       parent_selection_type=parent_selection_type,\n",
    "                       keep_parents=keep_parents, # Number of parents to keep from current population.\n",
    "                       # keep_elitism = 1, # The number of the solutions with the best fitness that will be kept for next generation.\n",
    "                       crossover_type=crossover_type,\n",
    "                       mutation_type=mutation_type,\n",
    "                       mutation_by_replacement=True, # If the previous gene is replaced or not.\n",
    "                       mutation_percent_genes=mutation_percent_genes, # The probability that each gene will be mutated\n",
    "                       # crossover_type=crossover_func, Can be used to customize a crossover func.\n",
    "                       # mutation_type=mutation_func, Can be used to customize a mutation func.\n",
    "                       )\n",
    "\n",
    "ga_instance.run()\n",
    "print('--------------------------------------------------')\n",
    "print(f'Generation: {num_generations}')\n",
    "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "print(\"Parameters of the best solution : {solution}\".format(solution=solution))\n",
    "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))\n",
    "summed_rows_out = np.sum(potential_solutions, axis=1)\n",
    "solution_to_check_out = np.multiply(summed_rows_out, solution)\n",
    "output_out = np.sum(solution_to_check_out * function_weights)\n",
    "print(\"Predicted output based on the best solution : {prediction}\".format(prediction=output_out))\n"
   ],
   "id": "e69ec6e51c9fa814",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------\n",
      "Generation: 1\n",
      "Parameters of the best solution : [5.36901268 9.69791802 9.74513585 3.4208652  9.02175631 1.56694692\n",
      " 5.06362429 9.60525948 4.20334316 1.48336083 2.48495632 8.45101528\n",
      " 2.19854153 6.65345884 2.29372961]\n",
      "Fitness value of the best solution = 0.0033418051313543356\n",
      "Predicted output based on the best solution : -1146.303393893933\n"
     ]
    }
   ],
   "execution_count": 55
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
