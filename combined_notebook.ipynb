{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "!pip install ucimlrepo\n",
    "!pip install pygad"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pygad\n",
    "from ucimlrepo import fetch_ucirepo\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "from sklearn.feature_selection import SelectKBest\n"
   ],
   "id": "2bec33f7cae768a4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ISOLET_DB_INDEX: int = 54\n",
    "SPAMBASE_DB_INDEX: int = 94"
   ],
   "id": "775d6e9179d6b50f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Read first dataset\n",
    "isolet = fetch_ucirepo(id=52) # Using this one cause it works.\n",
    "\n",
    "features = isolet.data.features\n",
    "target_variables = isolet.data.targets"
   ],
   "id": "8e3653a86502cc81",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Print missing values and maximum and minimum values in the features of the first dataset\n",
    "X_df = pd.DataFrame(features)\n",
    "y_df = pd.DataFrame(target_variables)\n",
    "\n",
    "print(\"Missing values in X:\", X_df.isnull().sum().sum())\n",
    "print(\"Missing values in y:\", y_df.isnull().sum().sum())\n",
    "\n",
    "print(\"Minimum value across all features:\", X_df.min().min())\n",
    "print(\"Maximum value across all features:\", X_df.max().max())"
   ],
   "id": "5d33b31bad0421c1",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Normalize the first dataset\n",
    "scaler = MinMaxScaler()\n",
    "X_scaled = scaler.fit_transform(X_df)\n",
    "\n",
    "X_scaled_df = pd.DataFrame(X_scaled, columns=X_df.columns)\n",
    "\n",
    "print(X_scaled_df.min().min())\n",
    "print(X_scaled_df.max().max())"
   ],
   "id": "64313dee7d6054df",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "data_after_scaling = X_scaled_df\n",
    "target_variables = y_df.values.ravel()"
   ],
   "id": "88c0991ca7f0b398",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:15:29.932568Z",
     "start_time": "2025-01-24T21:15:29.929122Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def get_top_k_features(selector,\n",
    "                       feature_names: list,\n",
    "                       top_features_to_select: int,\n",
    "                       algorithm: str,\n",
    "                       verbose: bool = False,\n",
    "                       normalize_score: bool = True):\n",
    "  \"\"\"\n",
    "  Get the top k features based on their scores from a SelectKBest selector.\n",
    "\n",
    "  Parameters:\n",
    "  selector (SelectKBest): Fitted SelectKBest object.\n",
    "  feature_names (list): List of feature names (columns of X).\n",
    "  k (int): Number of top features to select.\n",
    "  algorithm (str): The name of the feature selection algorithm.\n",
    "\n",
    "  Returns:\n",
    "  A dataframe that contains 2 columns: The first is \"Feature\" and is the feature name and the second is a score, normalization is dependent on the var sent..\n",
    "\n",
    "  \"\"\"\n",
    "  # Retrieve feature scores\n",
    "  scores = selector.scores_\n",
    "\n",
    "  if normalize_score:\n",
    "      scores = scores / np.nansum(scores)\n",
    "\n",
    "  feature_ranking = pd.DataFrame({\n",
    "    'Feature': feature_names,\n",
    "    'Score': scores\n",
    "  }).sort_values(by='Score', ascending=False)\n",
    "  if verbose:\n",
    "    # Display top-ranked features\n",
    "    print(f\"Feature Rankings using {algorithm}:\")\n",
    "    print(feature_ranking)\n",
    "\n",
    "\n",
    "  # Return selected top k features\n",
    "  return feature_ranking.head(top_features_to_select)"
   ],
   "id": "641eabc4d83c5a34",
   "outputs": [],
   "execution_count": 71
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "RANDOM_FOREST_SEED: int = 42",
   "id": "d0071fc97b791c87",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def train_and_fit_random_forest(X_train, X_test, y_train, y_test):\n",
    "  \"\"\"\n",
    "  Builds, trains, and evaluates a Random Forest classification model.\n",
    "\n",
    "  Parameters:\n",
    "  ----------\n",
    "  X_train : pd.DataFrame or np.ndarray\n",
    "      Feature matrix for training the model.\n",
    "  X_test : pd.DataFrame or np.ndarray\n",
    "      Feature matrix for testing the model.\n",
    "  y_train : pd.Series or np.ndarray\n",
    "      Target labels for training the model.\n",
    "  y_test : pd.Series or np.ndarray\n",
    "      True target labels for testing the model.\n",
    "\n",
    "  Returns:\n",
    "  float: The accuracy of the model on the selected features\n",
    "  \"\"\"\n",
    "  # Build a simple classification model\n",
    "  model = RandomForestClassifier(random_state=RANDOM_FOREST_SEED)\n",
    "  model.fit(X_train, y_train)\n",
    "\n",
    "  # Make predictions\n",
    "  y_pred = model.predict(X_test)\n",
    "\n",
    "  # Evaluate the model\n",
    "  accuracy = accuracy_score(y_test, y_pred)\n",
    "  print(\"Model Accuracy:\", accuracy)\n",
    "\n",
    "  # Detailed performance metrics\n",
    "  # print(\"\\nClassification Report:\")\n",
    "  # print(classification_report(y_test, y_pred))\n",
    "\n",
    "  # Return the accuracy of the model\n",
    "  return accuracy\n"
   ],
   "id": "b8a8d5a8331edcf9",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def get_top_features_with_selector(selector: SelectKBest,\n",
    "                                   num_of_features_to_select: int,\n",
    "                                   data_with_features,\n",
    "                                   target_variables,\n",
    "                                   algorithm: str = \"\",\n",
    "                                   verbose: bool = False\n",
    "                                   ):\n",
    "      \"\"\"\n",
    "      :param selector: SelectKBest object.\n",
    "      :param num_of_features_to_select:\n",
    "      :param data_with_features: The features are selected from this data.\n",
    "      :param target_variables: The variable the feature selection is used on.\n",
    "      :param algorithm: The algorithm used, as a str. Used for debug printouts.\n",
    "      :param verbose: Enable debug printouts.\n",
    "      :return:\n",
    "      \"\"\"\n",
    "      selector.fit(data_with_features, target_variables)\n",
    "\n",
    "      # Rank the features using Chi-Square algorithm\n",
    "      top_features_with_scores = get_top_k_features(selector=selector, feature_names=features.columns,\n",
    "                                        top_features_to_select=num_of_features_to_select, algorithm=algorithm,\n",
    "                                        verbose=verbose)\n",
    "      top_features = top_features_with_scores['Feature'].tolist()\n",
    "      return data_with_features[top_features]"
   ],
   "id": "7dfb35ab03fc1301",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "MIN_FEATURES: int = 5\n",
    "MAX_FEATURES: int = 10\n",
    "TRAIN_TEST_SPLIT_RATIO: float = 0.2\n"
   ],
   "id": "60656a67680f31c3",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Import all the feature selection algorithms.",
   "id": "fedf31d48d2d5d3d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.feature_selection import mutual_info_classif\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.feature_selection import f_classif\n",
    "from sklearn.feature_selection import f_regression\n",
    "\n",
    "mutual_info_classif_with_random_state = lambda X, y: mutual_info_classif(X, y, random_state=42)\n",
    "mutual_info_regression_with_random_state = lambda X, y: mutual_info_regression(X, y, random_state=42)\n",
    "\n",
    "# Selects featuers based on the k best scores. Here k is 'all'.\n",
    "classifier_chi2: SelectKBest = SelectKBest(score_func=chi2, k='all')\n",
    "classifier_mutual_info_classif: SelectKBest = SelectKBest(score_func=mutual_info_classif_with_random_state, k='all')\n",
    "classifier_mutual_info_regression: SelectKBest = SelectKBest(score_func=mutual_info_regression_with_random_state, k='all')\n",
    "classifier_f_classif: SelectKBest = SelectKBest(score_func=f_classif, k='all')\n",
    "classifier_f_regression: SelectKBest = SelectKBest(score_func=f_regression, k='all')"
   ],
   "id": "99638d58bd84024",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Preselecting all the features with each classifier to get a feature ranking.",
   "id": "2a1efd3ac68487d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "FEATURES_TO_SELECT = 5",
   "id": "9715f9685b56b95c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:17:30.403918Z",
     "start_time": "2025-01-24T21:17:30.397437Z"
    }
   },
   "cell_type": "code",
   "source": [
    "top_features = get_top_k_features(\n",
    "    selector=classifier_chi2,\n",
    "    feature_names=features.columns,\n",
    "    top_features_to_select=FEATURES_TO_SELECT,\n",
    "    algorithm=\"classifier_chi2\",\n",
    ")\n",
    "top_features"
   ],
   "id": "b24d76539a0402be",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "        Feature     Score\n",
       "0    Attribute1  0.135475\n",
       "4    Attribute5  0.129577\n",
       "2    Attribute3  0.117055\n",
       "6    Attribute7  0.091419\n",
       "30  Attribute31  0.060236"
      ],
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Feature</th>\n",
       "      <th>Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Attribute1</td>\n",
       "      <td>0.135475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Attribute5</td>\n",
       "      <td>0.129577</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Attribute3</td>\n",
       "      <td>0.117055</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Attribute7</td>\n",
       "      <td>0.091419</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>Attribute31</td>\n",
       "      <td>0.060236</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ]
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 76
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-24T21:15:01.551376Z",
     "start_time": "2025-01-24T21:15:01.531314Z"
    }
   },
   "cell_type": "code",
   "source": [
    "selector_list = [\n",
    "    (classifier_chi2, \"classifier_chi2\"),\n",
    "    (classifier_mutual_info_classif, \"classifier_mutual_info_classif\"),\n",
    "    # (classifier_mutual_info_regression, \"classifier_mutual_info_regression\"),\n",
    "    # (classifier_f_classif, \"classifier_f_classif\"),\n",
    "    # (classifier_f_regression, \"classifier_f_regression\")\n",
    "]\n",
    "features_selected_by_each_algorithm: dict = dict()\n",
    "for selector_in_list in selector_list:\n",
    "    selector_in_list: tuple[SelectKBest, str]\n",
    "    selector_in_list[0].fit(data_after_scaling, target_variables)\n",
    "      # Rank the features using Chi-Square algorithm\n",
    "    top_features = get_top_k_features(\n",
    "        selector=selector_in_list[0],\n",
    "        feature_names=features.columns,\n",
    "        top_features_to_select=FEATURES_TO_SELECT,\n",
    "        algorithm=selector_in_list[1],\n",
    "    )\n",
    "    features_selected_by_each_algorithm[selector_in_list[1]] = top_features\n"
   ],
   "id": "f0b36a01d9e3141e",
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'DataFrame' object has no attribute 'tolist'",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m                            Traceback (most recent call last)",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_20720\\259886945.py\u001B[0m in \u001B[0;36m?\u001B[1;34m()\u001B[0m\n\u001B[0;32m      9\u001B[0m \u001B[1;32mfor\u001B[0m \u001B[0mselector_in_list\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mselector_list\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     10\u001B[0m     \u001B[0mselector_in_list\u001B[0m\u001B[1;33m:\u001B[0m \u001B[0mtuple\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mSelectKBest\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mstr\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     11\u001B[0m     \u001B[0mselector_in_list\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mfit\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mdata_after_scaling\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mtarget_variables\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     12\u001B[0m       \u001B[1;31m# Rank the features using Chi-Square algorithm\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 13\u001B[1;33m     top_features = get_top_k_features(\n\u001B[0m\u001B[0;32m     14\u001B[0m         \u001B[0mselector\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mselector_in_list\u001B[0m\u001B[1;33m[\u001B[0m\u001B[1;36m0\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     15\u001B[0m         \u001B[0mfeature_names\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mfeatures\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mcolumns\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     16\u001B[0m         \u001B[0mtop_features_to_select\u001B[0m\u001B[1;33m=\u001B[0m\u001B[0mFEATURES_TO_SELECT\u001B[0m\u001B[1;33m,\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n",
      "\u001B[1;32m~\\AppData\\Local\\Temp\\ipykernel_20720\\1242976192.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(selector, feature_names, top_features_to_select, algorithm, verbose, normalize_score)\u001B[0m\n\u001B[0;32m     32\u001B[0m     \u001B[0mprint\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mfeature_ranking\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     33\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     34\u001B[0m \u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m     35\u001B[0m   \u001B[1;31m# Return selected top k features\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m---> 36\u001B[1;33m   \u001B[1;32mreturn\u001B[0m \u001B[0mfeature_ranking\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mhead\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mtop_features_to_select\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0mtolist\u001B[0m\u001B[1;33m(\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;32m~\\miniconda3\\Lib\\site-packages\\pandas\\core\\generic.py\u001B[0m in \u001B[0;36m?\u001B[1;34m(self, name)\u001B[0m\n\u001B[0;32m   6295\u001B[0m             \u001B[1;32mand\u001B[0m \u001B[0mname\u001B[0m \u001B[1;32mnot\u001B[0m \u001B[1;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_accessors\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6296\u001B[0m             \u001B[1;32mand\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_info_axis\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m_can_hold_identifiers_and_holds_name\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6297\u001B[0m         \u001B[1;33m)\u001B[0m\u001B[1;33m:\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0;32m   6298\u001B[0m             \u001B[1;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[1;33m[\u001B[0m\u001B[0mname\u001B[0m\u001B[1;33m]\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[1;32m-> 6299\u001B[1;33m         \u001B[1;32mreturn\u001B[0m \u001B[0mobject\u001B[0m\u001B[1;33m.\u001B[0m\u001B[0m__getattribute__\u001B[0m\u001B[1;33m(\u001B[0m\u001B[0mself\u001B[0m\u001B[1;33m,\u001B[0m \u001B[0mname\u001B[0m\u001B[1;33m)\u001B[0m\u001B[1;33m\u001B[0m\u001B[1;33m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[1;31mAttributeError\u001B[0m: 'DataFrame' object has no attribute 'tolist'"
     ]
    }
   ],
   "execution_count": 70
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "function_weights = np.array([-4, 5, 7, 2.3, -2, 9, 9, -2, 5, 12, 57, 42, 42, 1, -3])\n",
    "desired_output = 100\n",
    "epsilon = 0.00001 # To prevent division by 0 errors.\n",
    "potential_solutions = np.random.randint(-10, 10, (len(function_weights),len(function_weights)))\n",
    "potential_solutions"
   ],
   "id": "8777a85c0b112906",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fitness_func_arr_of_weights(ga_instance, solution, solution_idx):\n",
    "    # Coded as an array of weights.\n",
    "    # TODO Here we can insert the feature selection and train the model, then classify, to measure fitness.\n",
    "    # TODO The higher fitness, the better.\n",
    "    summed_rows = np.sum(potential_solutions, axis=1)\n",
    "    solution_to_check = np.multiply(summed_rows, solution)\n",
    "    output = np.sum(solution_to_check * function_weights)\n",
    "    fitness = 1.0 / (np.abs(output - desired_output) + epsilon) #\n",
    "    return fitness\n"
   ],
   "id": "e03f8e73ff4e6507",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "for algorithm_name, algorithm_result in features_selected_by_each_algorithm.items():\n",
    "  algorithm_name: str\n",
    "  algorithm_result: dict\n",
    "  print(algorithm_result)"
   ],
   "id": "ef18fd4a7bf8415e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def fitness_func_as_weights_to_use_from_each_algorithm(ga_instance, solution, solution_idx):\n",
    "  for algorithm_name_and_result in features_selected_by_each_algorithm:\n",
    "      algorithm_name: tuple[SelectKBest, str]\n",
    "  X_train, X_test, y_train, y_test = train_test_split(X_top, target_variables, test_size=0.2, random_state=42)\n",
    "  # Train and fit random forest classification model based on feature selected\n",
    "  accuracy = train_and_fit_random_forest(X_train, X_test, y_train, y_test)"
   ],
   "id": "9701027316f75369",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "num_generations = 1\n",
    "num_parents_mating = 4\n",
    "\n",
    "sol_per_pop = 12\n",
    "num_genes = function_weights.size # Use this to control the number of feature selection potential solutions is used.\n",
    "\n",
    "init_range_low = 0\n",
    "init_range_high = 10\n",
    "\n",
    "parent_selection_type = \"sss\" #steady-state selection, meaning it selects the parents with the highest fitness.\n",
    "keep_parents = 1\n",
    "\n",
    "crossover_type = \"single_point\" # Swaps the chromosomes from a certain index onwards between the parents.\n",
    "\n",
    "mutation_type = \"random\"\n",
    "mutation_percent_genes = 20\n",
    "ga_instance = pygad.GA(num_generations=num_generations,\n",
    "                       num_parents_mating=num_parents_mating, # Num of parents to select each generation.\n",
    "                       fitness_func=fitness_func_arr_of_weights,\n",
    "                       sol_per_pop=sol_per_pop, # Number of solutions per population.\n",
    "                       num_genes=num_genes, # Effectively, the thing that is tweaked for each generation.\n",
    "                       # gene_type=list[float], # The type of gene, meaning of each value inside a chromosome. Supports list.\n",
    "                       init_range_low=init_range_low, # dependent on the gene type, the range of values to be generated.\n",
    "                       init_range_high=init_range_high,\n",
    "                       parent_selection_type=parent_selection_type,\n",
    "                       keep_parents=keep_parents, # Number of parents to keep from current population.\n",
    "                       # keep_elitism = 1, # The number of the solutions with the best fitness that will be kept for next generation.\n",
    "                       crossover_type=crossover_type,\n",
    "                       mutation_type=mutation_type,\n",
    "                       mutation_by_replacement=True, # If the previous gene is replaced or not.\n",
    "                       mutation_percent_genes=mutation_percent_genes, # The probability that each gene will be mutated\n",
    "                       # crossover_type=crossover_func, Can be used to customize a crossover func.\n",
    "                       # mutation_type=mutation_func, Can be used to customize a mutation func.\n",
    "                       )\n",
    "\n",
    "ga_instance.run()\n",
    "print('--------------------------------------------------')\n",
    "print(f'Generation: {num_generations}')\n",
    "solution, solution_fitness, solution_idx = ga_instance.best_solution()\n",
    "print(\"Parameters of the best solution : {solution}\".format(solution=solution))\n",
    "print(\"Fitness value of the best solution = {solution_fitness}\".format(solution_fitness=solution_fitness))\n",
    "summed_rows_out = np.sum(potential_solutions, axis=1)\n",
    "solution_to_check_out = np.multiply(summed_rows_out, solution)\n",
    "output_out = np.sum(solution_to_check_out * function_weights)\n",
    "print(\"Predicted output based on the best solution : {prediction}\".format(prediction=output_out))\n"
   ],
   "id": "e69ec6e51c9fa814",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
